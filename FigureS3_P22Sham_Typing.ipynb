{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from import_RNA import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gluta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22 = sc.read_h5ad('h5ads/sham_S1S2S3B2_analyzed.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22.X = P22.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P22_glut = P22.copy()\n",
    "for i in ['Astro', 'Endo', 'Lamp5', 'Micro', 'OD', 'OPC', 'Pvalb', 'Sst', 'VLMC', 'Vip']:\n",
    "    P22_glut = P22_glut[P22_glut.obs.Subclass!=i,:]\n",
    "pipeline_short(P22_glut, batch_correct=True, batch_ID='Sample',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(P22_glut, color=['Subclass', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '15,16', '20,21', '29,30',\n",
    "                                                                             '30,31'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(P22_glut,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_glut), color=['Sample', 'Subclass'])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.25, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(P22_glut, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(P22_glut, color=['Subclass','leiden_'+str(i)], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.X = P22_glut.raw.X\n",
    "genes = list(P22_glut[:,P22_glut.var.highly_variable].var_names)\n",
    "P22_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(P22_glut, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=P22_glut, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(P22_glut, obs))\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    \n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(P22_glut[P22_glut.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(P22_glut[P22_glut.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    #build F1 plot\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1)) # cluster, F1 score pairs\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1) #cluster's-subclass, F1 score pairs\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    a['Accuracy'] = acc_clusts\n",
    "    P22_dfs.append(a)\n",
    "    del a\n",
    "    #build acc plot\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(P22_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(P22_dfs), x='Res', y=\"Accuracy\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P21 = rem_fem(sc.read_h5ad('P21full_glut_v3.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(P21, color=['Study', 'Subclass', 'Type_nn_dists'], legend_loc='on data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items, mapping_items = pairwise_map(adata_t0=P22_glut, adata_t1=P21, test_lab='leiden', train_lab='Type_nn_dists', \n",
    "            t0_dict=make_dict(P22_glut, 'leiden'), t1_dict=make_dict(P21, 'Type_nn_dists'),min_cells=200,x_lab='P21 V1',\n",
    "            y_lab='P22 S1', union_hvgs=False, recomp_HVGs=False)\n",
    "\n",
    "test_labelsS1vsV1, test_predlabelsS1vsV1, test_prediction_S1vsV1 = test_items\n",
    "mappingconfmatS1vsV1, mappingxticksS1vsV1, mappingplotS1vsV1 = mapping_items\n",
    "\n",
    "P22_glut.obs['P21 Mapping Prob'] = np.max(test_prediction_S1vsV1, axis=1)\n",
    "P22_glut.obs['P21 Mapping Label'] = pd.Categorical(test_predlabelsS1vsV1).rename_categories(list(make_dict(P21, 'Type_nn_dists').keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_glut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_glut), color=['Sample', 'Subclass', ])\n",
    "sc.pl.umap(shuffle(P22_glut), color=['leiden_1', 'leiden_2', 'P21 Mapping Label', 'P21 Mapping Prob'], legend_loc='on data',\n",
    "          legend_fontsize=10)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(P22_glut, color=['Cdh13', 'Adamts2', 'Baz1a','Trpc6', 'Agmat','Rrad','Chrm2'], \n",
    "           color_map='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(P22_glut, var_names=['Cdh13', 'Adamts2', 'Baz1a','Trpc6', 'Agmat','Rrad','Chrm2', 'Mcc'], \n",
    "              groupby='P21 Mapping Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in reses:\n",
    "    \n",
    "    fig, axs = plt.subplots(1,1,figsize=(i*0.8*12, 6))\n",
    "    hmap_func = make_hmap(P22_glut, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':8},ax=axs,)\n",
    "    plt.title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    sc.pl.umap(shuffle(P22_glut), color=['leiden_'+str(i), 'Subclass', 'P21 Mapping Label', 'P21 Mapping Prob'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(P22_glut, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':12},ax=axs[a,b],)\n",
    "    axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.write_h5ad('h5ads/P22Sham_glut_v1.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut = sc.read_h5ad('h5ads/P22Sham_glut_v1.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23 = P22_glut[P22_glut.obs['Subclass']=='L2/3',:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(L23, 'P21 Mapping Prob','P21 Mapping Label', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L23.X = L23.raw.X\n",
    "for i in L23.obs['P21 Mapping Label'].value_counts()[3:].index:\n",
    "    L23 = L23[L23.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(L23, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(L23, color=['P21 Mapping Label', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '20,21'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(L23,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "del L23.obsp\n",
    "sc.pp.neighbors(L23, use_rep='X_harmony',n_pcs=20, )\n",
    "sc.tl.umap(L23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(L23, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(L23, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(L23, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23.X = L23.raw.X\n",
    "genes = list(L23[:,L23.var.highly_variable].var_names)\n",
    "L23_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(L23, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=L23, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(L23, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(L23[L23.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(L23[L23.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    a['Accuracy'] = acc_clusts\n",
    "\n",
    "    L23_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L23_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(L23, color=['leiden_0.3', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Cdh13', 'Adamts2', 'Trpc6', 'Baz1a', 'Agmat', 'Chrm2', 'Ncam2'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(L23, color=['leiden_0.3', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Cdh13', 'Adamts2', 'Trpc6', 'Baz1a', 'Agmat', 'Chrm2'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L23, var_names=['Trpc6', 'Baz1a', 'Npas4', 'Agmat','Chrm2', 'Ncam2', 'Cdh13', 'Adamts2', ],\n",
    "             groupby='leiden_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L23_marks = []\n",
    "for i in L23.obs['leiden_0.3'].values.categories:\n",
    "    L23_marks.append(DE(L23,obs_id='leiden_0.3', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Baz1a' in L23_marks[0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=L23, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.3', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23.obs['leiden_0.3'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23.obs['Type'] = L23.obs['leiden_0.3'].cat.rename_categories(new_categories=['L2/3_B', 'L2/3_C', 'L2/3_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23.write_h5ad('h5ads/P22Sham_typing/P22Sham_L23_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L23, color=['Type', 'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L23_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(L23, var_names=i.index[0:20], groupby='Type', dendrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L23, var_names=['Fosb', 'Fosl2', 'Nr4a3', 'Egr2',\n",
    "                             'Nr4a2', 'Nptx2', 'Fos'], \n",
    "              groupby='Type', dendrogram=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condylis et al IEGs in B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(L23, keys=['Fosb', 'Fosl2', 'Nr4a3', 'Egr2',\n",
    "                             'Nr4a2', 'Nptx2', 'Fos'], \n",
    "              groupby='Type', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23.obs.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L23_marks[2][0:40].index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4 = P22_glut[P22_glut.obs['Subclass']=='L4',:].copy()\n",
    "L4.X = L4.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L4.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(L4, 'P21 Mapping Prob','P21 Mapping Label', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L4.obs['P21 Mapping Label'].value_counts()[3:].index:\n",
    "    L4 = L4[L4.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(L4, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4 = sc.read_h5ad('h5ads/P22Sham_typing/P22Sham_L4_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(L4, color=['Bdnf'], legend_loc='on data', \n",
    "               legend_fontsize=10)\n",
    "sc.pl.dotplot(L4, var_names=['Bdnf'], groupby='leiden_0.4')\n",
    "sc.pl.dotplot(L4, var_names=['Bdnf'], groupby='leiden_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(L4, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(L4, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(L4, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4.X = L4.raw.X\n",
    "genes = list(L4[:,L4.var.highly_variable].var_names)\n",
    "L4_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(L4, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=L4, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(L4, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(L4[L4.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(L4[L4.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    L4_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L4_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L23_dfs+L4_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(L4), color=['leiden_0.2', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Nell1', 'Etv6', 'Il1rapl2', \n",
    "                      'Rspo1', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(L4, color=['leiden_0.2', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Nell1', 'Etv6', 'Il1rapl2', \n",
    "                      'Rspo1', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(L4, color=['leiden_0.2',],\n",
    "               legend_loc='on data',components=['1,2', '2,3', '3,4', '4,5', '5,6', '6,7'],\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L4, var_names=['Il1rapl2','Nell1', 'Etv6',   'Rspo1', ],\n",
    "             groupby='leiden_0.3', dendrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L4_marks = []\n",
    "for i in L4.obs['leiden_0.2'].values.categories:\n",
    "    L4_marks.append(DE(L4,obs_id='leiden_0.2', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))\n",
    "for i in L4_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(L4, var_names=i.index[0:10], groupby='leiden_0.2', dendrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=L4, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.2', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L4.obs['leiden_0.2'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4.obs['Type'] = L4.obs['leiden_0.2'].cat.rename_categories(new_categories=['L4_A', 'L4_B', 'L4_C',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L4, color=['Type', 'leiden_0.2', 'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4.write_h5ad('h5ads/P22Sham_typing/P22Sham_L4_types.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4_V1 = P21[P21.obs.Subclass=='L4',:].copy()\n",
    "L4_V1 = L4_V1[L4_V1.obs.Study!='2023 Multiome',:].copy()\n",
    "L4_V1.X = L4_V1.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4_mix = L4.concatenate(L4_V1, batch_categories=['S1', 'V1'], batch_key='Study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_short(L4_mix, batch_correct=False, batch_ID=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L4_mix, color=['Sample', 'Study', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(L4_mix, color=['Type', 'Study'],\n",
    "               legend_loc='on data',components=['1,2', '2,3', '4,5', '5,6'],\n",
    "          legend_fontsize=10, color_map='hot', basis='X_pca')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4_mix.X = L4_mix.raw.X\n",
    "pipeline_short(L4_mix, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(L4_mix), color=['Sample', 'Study', 'Type', 'leiden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(L4_mix, color=['Type', 'Study'],\n",
    "               legend_loc='on data',components=['1,2', '2,3', '4,5', '5,6', '6,7', '7,8', '8,9', '9,10'],\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=L4_mix, a_=4, b_=3, samp_id='Study', clus_id='leiden', size=(13,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L6CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6CT = P22_glut[P22_glut.obs['Subclass']=='L6CT',:].copy()\n",
    "L6CT.X = L6CT.raw.X\n",
    "L6CT.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L6CT.obs['P21 Mapping Label'].value_counts()[3:].index:\n",
    "    L6CT = L6CT[L6CT.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(L6CT, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(L6CT, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(L6CT, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(L6CT, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6CT.X = L6CT.raw.X\n",
    "genes = list(L6CT[:,L6CT.var.highly_variable].var_names)\n",
    "L6CT_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(L6CT, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=L6CT, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(L6CT, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(L6CT[L6CT.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(L6CT[L6CT.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    L6CT_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L23_dfs+L4_dfs+L6CT_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(L6CT, color=['leiden_0.3', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Galntl6', 'Brinp3', 'Htr4'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(L6CT, color=['leiden_0.3', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob','Galntl6', 'Brinp3', 'Htr4'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(L6CT, color=['leiden_0.3',],\n",
    "               legend_loc='on data',components=['1,2', '2,3', '3,4', '4,5'],\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L6CT = L6CT[L6CT.obs['leiden_0.3']!='3',:]\n",
    "L6CT = L6CT[L6CT.obs['leiden_0.3']!='4',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L6CT, var_names=[ 'Brinp3', 'Htr4','Galntl6',],\n",
    "             groupby='leiden_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L6CT_marks = []\n",
    "for i in L6CT.obs['leiden_0.3'].values.categories:\n",
    "    L6CT_marks.append(DE(L6CT,obs_id='leiden_0.3', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))\n",
    "for i in L6CT_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(L6CT, var_names=i.index[0:10], groupby='leiden_0.3', dendrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_obs_plot(L6CT, 'P21 Mapping Prob', obs_id_split='leiden_0.3')\n",
    "clust_obs_plot(L6CT, 'pct_counts_mt', obs_id_split='leiden_0.3')\n",
    "clust_obs_plot(L6CT, 'Doublet Score', obs_id_split='leiden_0.3')\n",
    "clust_obs_plot(L6CT, 'total_counts', obs_id_split='leiden_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=L6CT, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.3', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6CT.obs['leiden_0.3'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6CT.obs['Type'] = L6CT.obs['leiden_0.3'].cat.rename_categories(new_categories=['L6CT_B', 'L6CT_C', 'L6CT_A', \n",
    "                                                                                'L6CT_D', 'L6CT_E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L6CT, color=['Type', 'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6CT.write_h5ad('h5ads/P22Sham_typing/P22Sham_L6CT_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6CT = sc.read_h5ad('h5ads/P22Sham_typing/P22Sham_L6CT_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L6CT, var_names=['Arhgap31', 'Pdlim1', 'Fam241a', 'Brinp3', 'Ust', 'Nell1',], groupby='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L56IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT = P22_glut[P22_glut.obs['Subclass']=='L5IT',:].concatenate(P22_glut[P22_glut.obs['Subclass']=='L6IT',:]).copy()\n",
    "L56IT.X = L56IT.raw.X\n",
    "L56IT.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L56IT.obs['P21 Mapping Label'].value_counts()[3:].index:\n",
    "    L56IT = L56IT[L56IT.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(L56IT, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(L56IT, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(L56IT, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT = L56IT[L56IT.obs['leiden_0.4']!='5',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(L56IT, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(L56IT, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(L56IT, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT.X = L56IT.raw.X\n",
    "genes = list(L56IT[:,L56IT.var.highly_variable].var_names)\n",
    "L56IT_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(L56IT, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=L56IT, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(L56IT, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(L56IT[L56IT.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(L56IT[L56IT.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    L56IT_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L56IT_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L23_dfs+L4_dfs+L56IT_dfs+L6CT_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(L56IT, color=['leiden_0.4', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Deptor', 'Etv1', 'Pparg', 'Pld5'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(L56IT, color=['leiden_0.4', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob','Deptor', 'Etv1', 'Pparg', 'Pld5'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(L56IT, color=['leiden_0.4',],\n",
    "               legend_loc='on data',components=['1,2', '2,3', '3,4', '4,5', '5,6'],\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L56IT, var_names=['Deptor', 'Etv1', 'Pparg', 'Pld5'],\n",
    "             groupby='leiden_0.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT_marks = []\n",
    "for i in L56IT.obs['leiden_0.4'].values.categories:\n",
    "    L56IT_marks.append(DE(L56IT,obs_id='leiden_0.4', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L56IT_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(L56IT, var_names=i.index[0:10], groupby='leiden_0.4', dendrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=L56IT, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.4', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT.obs['leiden_0.4'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT.obs['Type'] = L56IT.obs['leiden_0.4'].cat.rename_categories(new_categories=['L5IT', 'L6IT_A', \n",
    "                                                                                  'L6IT_B', 'L6IT_Car3',\n",
    "                                                                                  'L6IT_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L56IT, color=['Type', 'leiden_0.4', 'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L56IT.write_h5ad('h5ads/P22Sham_typing/P22Sham_L56IT_types.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L5NP/PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L5NPT = P22_glut[P22_glut.obs['Subclass']=='L5NP',:].concatenate(P22_glut[P22_glut.obs['Subclass']=='L5PT',:]).copy()\n",
    "L5NPT.X = L5NPT.raw.X\n",
    "L5NPT.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in L5NPT.obs['P21 Mapping Label'].value_counts()[3:].index:\n",
    "    L5NPT = L5NPT[L5NPT.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(L5NPT, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(L5NPT, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(L5NPT, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(L5NPT, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L5NPT.X = L5NPT.raw.X\n",
    "genes = list(L5NPT[:,L5NPT.var.highly_variable].var_names)\n",
    "L5NPT_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(L5NPT, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=L5NPT, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(L5NPT, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(L5NPT[L5NPT.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(L5NPT[L5NPT.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    L5NPT_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L5NPT_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(L23_dfs+L4_dfs+L5NPT_dfs+L56IT_dfs+L6CT_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(L5NPT, color=['leiden_0.7', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', 'Tshz2', 'Qrfpr', 'Fn1'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=40)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(L5NPT, color=['leiden_0.7', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',  'Tshz2', 'Qrfpr', 'Fn1'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(L56IT, color=['leiden_0.7',],\n",
    "               legend_loc='on data',components=['1,2', '2,3', '3,4', '4,5', '5,6'],\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L5NPT, var_names=[ 'Tshz2', 'Qrfpr', 'Fn1', ],\n",
    "             groupby='leiden_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.dendrogram(L5NPT, groupby='leiden_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L5NPT_marks = []\n",
    "for i in L5NPT.uns[\"dendrogram_['leiden_0.7']\"]['categories_ordered']:\n",
    "    L5NPT_marks.append(DE(L5NPT,obs_id='leiden_0.7', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They just have to be DE within each dendrogram clade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in L5NPT_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(L5NPT, var_names=i.index[0:10], groupby='leiden_0.7', dendrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=L5NPT, a_=4, b_=3, samp_id='Sample', clus_id='leiden_0.7', size=(13,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L5NPT.obs['leiden_0.7'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L5NPT, color=['leiden_0.7', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', size=40)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = ['L5NP_A', 'L5PT_A', 'L5PT_B', 'L5PT_C', 'L5NP_B', 'L5PT_D', 'L5PT_E', 'L5PT_F', 'L5PT_G', 'L5NP_C']\n",
    "L5NPT.obs['Type'] = L5NPT.obs['leiden_0.7'].cat.rename_categories(new_categories=news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(L5NPT, color=['Type', 'P21 Mapping Label', 'leiden_0.7'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L5NPT.write_h5ad('h5ads/P22Sham_typing/P22Sham_L5NPT_types.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Type assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed during subclass-specific cell typing:\n",
    "\n",
    "**A few hundred cells in every section that were mapping to something else when mapped to P21**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6b = P22_glut[P22_glut.obs.Subclass=='L6b',:]\n",
    "L6b.obs['Type'] = pd.Categorical(L6b.shape[0]*['L6b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut_types_list = []\n",
    "for i in ['P22Sham_L23_types', 'P22Sham_L4_types', 'P22Sham_L56IT_types', 'P22Sham_L5NPT_types',\n",
    "         'P22Sham_L6CT_types']:\n",
    "    P22_glut_types_list.append(sc.read_h5ad('h5ads/P22Sham_typing/'+i+'.h5ad'))\n",
    "\n",
    "a = P22_glut_types_list+[L6b]\n",
    "\n",
    "P22_glut_types = a[0].concatenate(a[1:],\n",
    "                                  batch_categories=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P22_glut_types.shape, P22_glut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_short(P22_glut_types, True, 'Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut_types.uns['Type_colors'] = ['#a1c9f4', '#ffb482', '#8de5a1', '#ff9f9b', \n",
    "                                     '#d0bbff', '#debb9b', '#fab0e4', '#cfcfcf',\n",
    "                                     '#fffea3', '#b9f2f0', \n",
    "                                    '#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', \n",
    "                                     '#a6d854', '#ffd92f', '#e5c494', '#b3b3b3',\n",
    "                                    '#f77189', '#bb9832', '#50b131', '#36ada4', '#3ba3ec', '#e866f4',\n",
    "                                    \n",
    "                                    '#f77189', '#bb9832', '#50b131', '#36ada4', '#3ba3ec', '#e866f4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_glut_types), color=['Sample'],  )\n",
    "sc.pl.umap(P22_glut_types, color=['Type', 'Subclass', 'leiden'], legend_loc='on data', legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.correlation_matrix(L5NPT, groupby='Type',show_correlation_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 11\n",
    "sc.pl.correlation_matrix(P22_glut_types, groupby='Type',show_correlation_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut_types.write_h5ad('h5ads/P22Sham_glut_v2.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut_types = sc.read_h5ad('h5ads/P22Sham_glut_v2.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FXR1P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FXR1P, and FXR2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GABA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22.obs.Subclass.values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba = P22.copy()\n",
    "for i in ['Astro', 'Endo', 'L2/3', 'L4', 'L5IT', 'L5NP', 'L5PT', 'L6CT', 'L6IT',\n",
    "       'L6b', 'Micro', 'OD', 'OPC', 'VLMC']:\n",
    "    P22_gaba = P22_gaba[P22_gaba.obs.Subclass!=i,:]\n",
    "pipeline_short(P22_gaba, batch_correct=True, batch_ID='Sample',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(P22_gaba, color=['Subclass', 'Fos', 'Egr1'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '15,16', '20,21', '29,30', '30,31'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(P22, keys=['Fos', 'Egr1', 'Pvalb', 'Cux2', 'Ccbe1'], groupby='Subclass', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(P22_gaba, color=['Subclass', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '15,16', '20,21', '29,30', '30,31'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(P22_gaba, color=['Sample', 'Subclass', 'Meis2', 'Pax6', 'Ccnd1',\n",
    "                           'Doublet Score', 'pct_counts_mt'])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.25, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(P22_gaba, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(P22_gaba, color=['Subclass','leiden_'+str(i)], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba.X = P22_gaba.raw.X\n",
    "genes = list(P22_gaba[:,P22_gaba.var.highly_variable].var_names)\n",
    "P22_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(P22_gaba, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=P22_gaba, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(P22_gaba, obs))\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    \n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(P22_gaba[P22_gaba.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(P22_gaba[P22_gaba.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    #build F1 plot\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1)) # cluster, F1 score pairs\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1) #cluster's-subclass, F1 score pairs\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    a['Accuracy'] = acc_clusts\n",
    "    P22_dfs.append(a)\n",
    "    del a\n",
    "    #build acc plot\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(P22_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(P22_dfs), x='Res', y=\"Accuracy\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P21 = rem_fem(sc.read_h5ad('P21full_gaba_v3.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(P21, color=['Study', 'Subclass', 'Type_nn_dists'], legend_loc='on data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items, mapping_items = pairwise_map(adata_t0=P22_gaba, adata_t1=P21, test_lab='leiden', train_lab='Type_nn_dists', \n",
    "            t0_dict=make_dict(P22_gaba, 'leiden'), t1_dict=make_dict(P21, 'Type_nn_dists'),min_cells=200,x_lab='P21 V1',\n",
    "            y_lab='P22 S1', union_hvgs=False, recomp_HVGs=False)\n",
    "\n",
    "test_labelsS1vsV1, test_predlabelsS1vsV1, test_prediction_S1vsV1 = test_items\n",
    "mappingconfmatS1vsV1, mappingxticksS1vsV1, mappingplotS1vsV1 = mapping_items\n",
    "\n",
    "P22_gaba.obs['P21 Mapping Prob'] = np.max(test_prediction_S1vsV1, axis=1)\n",
    "P22_gaba.obs['P21 Mapping Label'] = pd.Categorical(test_predlabelsS1vsV1).rename_categories(list(make_dict(P21, 'Type_nn_dists').keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_gaba), color=['Sample', 'Subclass', ])\n",
    "sc.pl.umap(shuffle(P22_gaba), color=['leiden_1', 'leiden_2', 'P21 Mapping Label', 'P21 Mapping Prob'], legend_loc='on data',\n",
    "          legend_fontsize=10)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in reses:\n",
    "    \n",
    "    fig, axs = plt.subplots(1,1,figsize=(i*0.8*12, 6))\n",
    "    hmap_func = make_hmap(P22_gaba, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':8},ax=axs,)\n",
    "    plt.title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    sc.pl.umap(shuffle(P22_gaba), color=['leiden_'+str(i), 'Subclass', 'P21 Mapping Label', 'P21 Mapping Prob'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(P22_gaba, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':12},ax=axs[a,b],)\n",
    "    axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba.write_h5ad('h5ads/P22Sham_gaba_v1.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pvalb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb = P22_gaba[P22_gaba.obs['Subclass']=='Pvalb',:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(Pvalb, 'P21 Mapping Prob','P21 Mapping Label', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb.obs['P21 Mapping Label'].value_counts()[4:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pvalb.X = Pvalb.raw.X\n",
    "for i in Pvalb.obs['P21 Mapping Label'].value_counts()[4:].index:\n",
    "    Pvalb = Pvalb[Pvalb.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(Pvalb, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Pvalb, color=['P21 Mapping Label', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '20,21'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(Pvalb,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "del Pvalb.obsp\n",
    "sc.pp.neighbors(Pvalb, use_rep='X_harmony',n_pcs=20, )\n",
    "sc.tl.umap(Pvalb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(Pvalb, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(Pvalb, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(Pvalb, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb = sc.read_h5ad('h5ads/P22Sham_typing/P22Sham_Pvalb_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb.X = Pvalb.raw.X\n",
    "genes = list(Pvalb[:,Pvalb.var.highly_variable].var_names)\n",
    "Pvalb_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(Pvalb, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=Pvalb, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(Pvalb, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(Pvalb[Pvalb.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(Pvalb[Pvalb.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    a['Accuracy'] = acc_clusts\n",
    "\n",
    "    Pvalb_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Pvalb_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(Pvalb, color=['leiden_0.4', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(Pvalb, color=['leiden_0.4', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pvalb_marks = []\n",
    "for i in Pvalb.obs['leiden_0.4'].values.categories:\n",
    "    Pvalb_marks.append(DE(Pvalb,obs_id='leiden_0.4', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Pvalb_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(Pvalb, var_names=i.index[0:10], groupby='leiden_0.4', dendrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(Pvalb, var_names=['Pvalb', 'Vipr2', 'Tpbg', 'Reln', 'Tac1'], groupby='leiden_0.4', dendrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=Pvalb, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.4', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb.obs['leiden_0.4'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb.obs['Type'] = Pvalb.obs['leiden_0.4'].cat.rename_categories(new_categories=['Pvalb_A', \n",
    "                                                                                  'Pvalb_B',\n",
    "                                                                                  'Pvalb_D',\n",
    "                                                                                 'Pvalb_C', \n",
    "                                                                                 'Pvalb_E',\n",
    "                                                                                 'Pvalb_F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pvalb.write_h5ad('h5ads/P22Sham_typing/P22Sham_Pvalb_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Pvalb, color=['Type','P21 Mapping Label', 'leiden_0.4'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in Pvalb_marks:\n",
    "    print(i.shape)\n",
    "    sc.pl.dotplot(Pvalb, var_names=i.index[0:20], groupby='Type', dendrogram=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(Pvalb, var_names=['Pvalb', 'Vipr2', 'Tpbg', 'Reln', 'Tac1'], groupby='Type', dendrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Pvalb.obs.Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in Pvalb_marks[-2][0:40].index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba = sc.read_h5ad('h5ads/P22Sham_gaba_v1.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sst = P22_gaba[P22_gaba.obs['Subclass']=='Sst',:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(Sst, 'P21 Mapping Prob','P21 Mapping Label', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sst.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Sst.X = Sst.raw.X\n",
    "for i in Sst.obs['P21 Mapping Label'].value_counts()[5:].index:\n",
    "    Sst = Sst[Sst.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(Sst, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Sst, color=['P21 Mapping Label', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '20,21'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(Sst,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(Sst, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(Sst, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(Sst, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sst.X = Sst.raw.X\n",
    "genes = list(Sst[:,Sst.var.highly_variable].var_names)\n",
    "Sst_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(Sst, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=Sst, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(Sst, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(Sst[Sst.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(Sst[Sst.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    a['Accuracy'] = acc_clusts\n",
    "\n",
    "    Sst_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Sst_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Pvalb_dfs+Sst_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7 to 1.0 all give the same clustering, so choose 0.9 b/c it has the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(Sst, color=['leiden_0.9', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Sst, color=['leiden_0.9', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sst_marks = []\n",
    "for i in Sst.obs['leiden_0.9'].values.categories:\n",
    "    Sst_marks.append(DE(Sst,obs_id='leiden_0.9', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Sst_marks:\n",
    "    sc.pl.dotplot(Sst, var_names=i.index[0:30], groupby='leiden_0.9', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Sst, color=['leiden_0.9', 'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=Sst, a_=4, b_=3, samp_id='Sample', clus_id='leiden_0.9', size=(13,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sst.obs['leiden_0.9'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sst.obs['Type'] = Sst.obs['leiden_0.9'].cat.rename_categories(new_categories=['Sst_A', 'Sst_B', 'Sst_C', 'Sst_D', \n",
    "                                                                              'Sst_E', 'Sst_F', 'Sst_G', 'Sst_H', \n",
    "                                                                              'Sst_I', 'Sst_J'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sst.write_h5ad('h5ads/P22Sham_typing/P22Sham_Sst_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Sst, color=['Type','P21 Mapping Label', 'leiden_0.9'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip = P22_gaba[P22_gaba.obs['Subclass']=='Vip',:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(Vip, 'P21 Mapping Prob','P21 Mapping Label', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Vip.X = Vip.raw.X\n",
    "for i in Vip.obs['P21 Mapping Label'].value_counts()[5:].index:\n",
    "    Vip = Vip[Vip.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(Vip, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Vip, color=['P21 Mapping Label', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '20,21'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(Vip,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(Vip, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(Vip, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Vip, color=['Subclass','leiden_0.2', 'P21 Mapping Label', 'P21 Mapping Prob'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(Vip, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip.X = Vip.raw.X\n",
    "genes = list(Vip[:,Vip.var.highly_variable].var_names)\n",
    "Vip_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(Vip, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=Vip, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(Vip, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(Vip[Vip.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(Vip[Vip.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    a['Accuracy'] = acc_clusts\n",
    "\n",
    "    Vip_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Vip_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Pvalb_dfs+Sst_dfs+Vip_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(Vip, color=['leiden_0.2', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Vip, color=['leiden_0.2', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Vip_marks = []\n",
    "for i in Vip.obs['leiden_0.2'].values.categories:\n",
    "    Vip_marks.append(DE(Vip,obs_id='leiden_0.2', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))\n",
    "    \n",
    "for i in Vip_marks:\n",
    "    sc.pl.dotplot(Vip, var_names=i.index[0:30], groupby='leiden_0.2', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=Vip, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.2', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip.obs['leiden_0.2'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip.obs['Type'] = Vip.obs['leiden_0.2'].cat.rename_categories(new_categories=['Vip_A', 'Vip_B', 'Vip_C', \n",
    "                                                                              'Vip_D', 'Vip_E', 'Vip_F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip.write_h5ad('h5ads/P22Sham_typing/P22Sham_Vip_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Vip, color=['Type','leiden_0.2', 'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vip = sc.read_h5ad('h5ads/P22Sham_typing/P22Sham_Vip_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(Vip, var_names=['Chat'], groupby='leiden_0.2', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lamp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lamp5 = P22_gaba[P22_gaba.obs['Subclass']=='Lamp5',:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.violin(Lamp5, 'P21 Mapping Prob','P21 Mapping Label', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lamp5.obs['P21 Mapping Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lamp5.X = Lamp5.raw.X\n",
    "for i in Lamp5.obs['P21 Mapping Label'].value_counts()[2:].index:\n",
    "    Lamp5 = Lamp5[Lamp5.obs['P21 Mapping Label']!=i]\n",
    "pipeline_short(Lamp5, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Lamp5, color=['P21 Mapping Label', ],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony', components=['1,2', '2,3', '3,4',\n",
    "                                                                             '4,5','5,6','6,7', '7,8', '8,9',\n",
    "                                                                             '20,21'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(Lamp5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res vs F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    sc.tl.leiden(Lamp5, resolution=i, key_added='leiden_'+str(i))\n",
    "    sc.pl.umap(Lamp5, color=['Subclass','leiden_'+str(i), 'P21 Mapping Label'], legend_loc='on data', \n",
    "               legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,3,figsize=(30, 20))\n",
    "a,b=0,0\n",
    "reses = [0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 1.25, 1.5, 1.75, 2]\n",
    "for i in reses:\n",
    "    \n",
    "    hmap_func = make_hmap(Lamp5, thres=0.01, y_obs='P21 Mapping Label', x_obs='leiden_'+str(i))\n",
    "\n",
    "    sn.heatmap(hmap_func[0], \n",
    "                   xticklabels=True, yticklabels=True, cmap='Reds',annot=True, \n",
    "                   annot_kws={'size':10},ax=axs[a,b],)\n",
    "    #axs[a,b].set_title('ARI = '+str(hmap_func[1])[0:4])\n",
    "    b = b +1\n",
    "    \n",
    "    if (b>2):\n",
    "        b = 0\n",
    "        a = a + 1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lamp5.X = Lamp5.raw.X\n",
    "genes = list(Lamp5[:,Lamp5.var.highly_variable].var_names)\n",
    "Lamp5_dfs = []\n",
    "for i in reses:\n",
    "    obs = 'leiden_'+str(i)\n",
    "    train_dic = make_dict(Lamp5, obs)\n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1, f1t1 = trainclassifier(train_anndata=Lamp5, \n",
    "                                                                                                     common_top_genes=genes, \n",
    "                                                                                                     obs_id=obs, \n",
    "                                                                                                     train_dict=train_dic, \n",
    "                                                                                                     eta=0.2,\n",
    "                                                                                                     max_cells_per_ident=1000, \n",
    "                                                                                              train_frac=0.7, \n",
    "                                                                                                     min_cells_per_ident=100)\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=make_dict(Lamp5, obs))\n",
    "    subs = []\n",
    "    for dic in train_dic:\n",
    "        #print(Lamp5[Lamp5.obs[obs]==i,:].obs['Subclass'].value_counts().index[0])\n",
    "        subs.append(Lamp5[Lamp5.obs[obs]==dic,:].obs['Subclass'].value_counts().index[0])\n",
    "\n",
    "    f1_dict_leiden = dict(zip(train_dic.keys(), f1t1))\n",
    "    f1_ser_subclass = pd.Series(index=subs,data=f1t1)\n",
    "\n",
    "    a = pd.DataFrame(f1_ser_subclass)\n",
    "    a['F1'] = a.values\n",
    "    a['Subclass'] = a.index\n",
    "    a['Res'] = a.shape[0]*[i]\n",
    "    \n",
    "    true = pd.Categorical(validation_label_train_70t0vst1)\n",
    "    pred = pd.Categorical(valid_predlabels_train_70t0vst1)\n",
    "\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    acc_clusts = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    \n",
    "    a['Accuracy'] = acc_clusts\n",
    "\n",
    "    Lamp5_dfs.append(a)\n",
    "    del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Lamp5_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(7,5))\n",
    "#plt.figure(figsize=(7,5))\n",
    "sn.pointplot(data=pd.concat(Pvalb_dfs+Sst_dfs+Vip_dfs+Lamp5_dfs), x='Res', y=\"F1\",hue='Subclass',capsize=.1, errwidth=1,)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc.pl.umap(Lamp5, color=['leiden_0.2', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot',)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.embedding(Lamp5, color=['leiden_0.2', 'Sample','Subclass', 'P21 Mapping Label', \n",
    "                        'P21 Mapping Prob',],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot', basis='X_harmony')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lamp5_marks = []\n",
    "for i in Lamp5.obs['leiden_0.2'].values.categories:\n",
    "    Lamp5_marks.append(DE(Lamp5,obs_id='leiden_0.2', obs_id_test=i, ref='rest', pts_thresh=0.2, lf_thresh=0.6))\n",
    "for i in Lamp5_marks:\n",
    "    sc.pl.dotplot(Lamp5, var_names=i.index[0:30], groupby='leiden_0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clus_sample_bars(adata=Lamp5, a_=3, b_=3, samp_id='Sample', clus_id='leiden_0.2', size=(13,9.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lamp5.obs['leiden_0.2'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lamp5.obs['Type'] = Lamp5.obs['leiden_0.2'].cat.rename_categories(new_categories=['Lamp5_A', 'Lamp5_B', 'Lamp5_C',\n",
    "                                                                                 'Lamp5_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lamp5.write_h5ad('h5ads/P22Sham_typing/P22Sham_Lamp5_types.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Lamp5, color=['Type','leiden_0.2',  'P21 Mapping Label'],\n",
    "               legend_loc='on data',\n",
    "          legend_fontsize=10, color_map='hot')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Type assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed during subclass-specific cell typing:\n",
    "\n",
    "**A few hundred cells in every section that were mapping to something else when mapped to P21**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba_types_list = []\n",
    "for i in ['P22Sham_Pvalb_types', 'P22Sham_Sst_types', 'P22Sham_Vip_types', 'P22Sham_Lamp5_types',]:\n",
    "    P22_gaba_types_list.append(sc.read_h5ad('h5ads/P22Sham_typing/'+i+'.h5ad'))\n",
    "\n",
    "\n",
    "P22_gaba_types = P22_gaba_types_list[0].concatenate(P22_gaba_types_list[1:],\n",
    "                                  batch_categories=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P22_gaba_types.shape, P22_gaba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_short(P22_gaba_types, True, 'Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba_types.uns['Type_colors'] = ['#a1c9f4', '#ffb482', '#8de5a1', '#ff9f9b', \n",
    "                                     '#d0bbff', '#debb9b', '#fab0e4', '#cfcfcf',\n",
    "                                     '#fffea3', '#b9f2f0', \n",
    "                                    '#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', \n",
    "                                     '#a6d854', '#ffd92f', '#e5c494', '#b3b3b3',\n",
    "                                    '#f77189', '#bb9832', '#50b131', '#36ada4', '#3ba3ec', '#e866f4',\n",
    "                                    \n",
    "                                    '#f77189', '#bb9832', '#50b131', '#36ada4', '#3ba3ec', '#e866f4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_gaba_types), color=['Sample'],  )\n",
    "sc.pl.umap(P22_gaba_types, color=['Type', 'Subclass', 'leiden'], legend_loc='on data', legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.correlation_matrix(Pvalb, groupby='Type',show_correlation_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 11\n",
    "sc.pl.correlation_matrix(P22_gaba_types, groupby='Type',show_correlation_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba_types.write_h5ad('h5ads/P22Sham_gaba_v2.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = pd.DataFrame(P22_gaba_types.obsm['X_umap'], index=P22_gaba_types.obs_names, columns=['UMAP1', 'UMAP2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba_types.obs['cluster'] = P22_gaba_types.obs['Type'].cat.rename_categories(np.arange(len(P22_gaba_types.obs['Type'].values.categories)))\n",
    "P22_gaba_types.obs['cluster_label'] = P22_gaba_types.obs['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = P22_gaba_types.obs['cluster']\n",
    "cl_label = P22_gaba_types.obs['cluster_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.to_csv('rd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.to_csv('cl.csv')\n",
    "cl_label.to_csv('cl_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd[rd.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.graph.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import hypergeom\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "\n",
    "def get_knn_graph(rd_dat, cl, cl_df, k=15, knn_outlier_th=2, outlier_frac_th=0.5):\n",
    "    knn_result = kneighbors_graph(rd_dat, n_neighbors=k, mode='connectivity').toarray()\n",
    "    knn = knn_result.astype(int)\n",
    "    knn_dist = knn_result * np.linalg.norm(rd_dat, axis=1)\n",
    "    cl_knn_dist_mean = {x: knn_dist[cl == x][:, 1:].mean() for x in np.unique(cl)}\n",
    "    cl_knn_dist_sd = {x: knn_dist[cl == x][:, 1:].std() for x in np.unique(cl)}\n",
    "    cl_knn_dist_th = {x: mean + knn_outlier_th * sd for x, mean, sd in zip(np.unique(cl), cl_knn_dist_mean.values(), cl_knn_dist_sd.values())}\n",
    "    \n",
    "    knn_dist_th = [cl_knn_dist_th[cl[int(i)]] for i in np.arange(len(knn))]\n",
    "    outlier = (knn_dist > np.array(knn_dist_th)[:, np.newaxis])\n",
    "    select_cells = np.array(range(len(outlier)))[outlier.sum(axis=1) < outlier_frac_th]\n",
    "    \n",
    "    pred_result = predict_knn(knn[select_cells], np.array(range(len(rd_dat))), cl)\n",
    "    pred_prob = pred_result['pred_prob']\n",
    "    knn_cell_cl_counts = np.round(pred_prob * knn.shape[1])\n",
    "    \n",
    "    knn_cl_cl_counts = np.zeros((len(np.unique(cl)), len(np.unique(cl))))\n",
    "    for i in np.unique(cl):\n",
    "        x = np.where(cl == i)[0]\n",
    "        knn_cl_cl_counts[i, :] = knn_cell_cl_counts[x, :].sum(axis=0)\n",
    "    \n",
    "    knn_cl_df = pd.DataFrame(knn_cl_cl_counts, columns=np.unique(cl))\n",
    "    knn_cl_df['cl.from'] = np.arange(len(np.unique(cl)))\n",
    "    knn_cl_df = knn_cl_df.melt(id_vars=['cl.from'], var_name='cl.to', value_name='Freq')\n",
    "    \n",
    "    from_size = knn_cl_cl_counts.sum(axis=1)\n",
    "    to_size = knn_cl_cl_counts.sum(axis=0)\n",
    "    total = knn_cl_cl_counts.sum()\n",
    "    \n",
    "    knn_cl_df['cl.from.total'] = from_size[knn_cl_df['cl.from']]\n",
    "    knn_cl_df['cl.to.total'] = to_size[knn_cl_df['cl.to']]\n",
    "    knn_cl_df = knn_cl_df[knn_cl_df['Freq'] > 0]\n",
    "    knn_cl_df['pval.log'] = knn_cl_df['odds'] = 0\n",
    "    \n",
    "    for i, row in knn_cl_df.iterrows():\n",
    "        q = row['Freq'] - 1\n",
    "        k = row['cl.from.total']\n",
    "        m = row['cl.to.total']\n",
    "        n = total - m\n",
    "        pval_log = hypergeom.logsf(q, M=n+m, n=n, N=k, loc=1)\n",
    "        knn_cl_df.at[i, 'pval.log'] = pval_log\n",
    "        odds = (q + 1) / (k * m / total)\n",
    "        knn_cl_df.at[i, 'odds'] = odds\n",
    "    \n",
    "    knn_cl_df['frac'] = knn_cl_df['Freq'] / knn_cl_df['cl.from.total']\n",
    "    knn_cl_df['cl.from.label'] = cl_df.loc[knn_cl_df['cl.from'], 'cluster_label'].values\n",
    "    knn_cl_df['cl.to.label'] = cl_df.loc[knn_cl_df['cl.to'], 'cluster_label'].values\n",
    "    \n",
    "    return {\n",
    "        'knn_result': knn_result,\n",
    "        'pred_result': pred_result,\n",
    "        'knn_cl_df': knn_cl_df\n",
    "    }\n",
    "\n",
    "def plot_constellation(knn_cl_df, cl_center_df, out_dir, node_label=\"cluster_id\", exxageration=2, curved=True,\n",
    "                       plot_parts=False, plot_hull=None, plot_height=25, plot_width=25, node_dodge=False,\n",
    "                       label_size=5, max_size=10, label_repel=False):\n",
    "    pass  # Your Python implementation of plot_constellation goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_knn_graph(rd_dat=rd, cl=cl, cl_df=cl_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Type refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut_types = sc.read_h5ad('h5ads_cmprsd/P22Sham_glut_v2.h5ad')\n",
    "P22_gaba_types = sc.read_h5ad('h5ads_cmprsd/P22Sham_gaba_v2.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_obs_plot(adata, obs_id, obs_id_split):\n",
    "\n",
    "    obs_quants = []\n",
    "    for i in adata.obs[obs_id_split].values.categories:\n",
    "    #for i in dr_corr.obs['Atlas Label'].values.categories:\n",
    "        obs_quants.append(np.mean(adata[adata.obs[obs_id_split]==i].obs[obs_id].values))\n",
    "        \n",
    "    x, y = adata.obs[obs_id_split].values.categories, obs_quants\n",
    "    ser = pd.Series(y,x).sort_values(ascending=False)\n",
    "    \n",
    "    x,y = ser.index, ser.values\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.bar(x,y)\n",
    "    plt.axhline(np.median(adata.obs[obs_id]), color='r')\n",
    "    plt.ylabel(obs_id)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_obs_plot(P22_glut_types, 'P21 Mapping Prob', obs_id_split='Type')\n",
    "clust_obs_plot(P22_glut_types, 'pct_counts_mt', obs_id_split='Type')\n",
    "clust_obs_plot(P22_glut_types, 'Doublet Score', obs_id_split='Type')\n",
    "clust_obs_plot(P22_glut_types, 'total_counts', obs_id_split='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_obs_plot(P22_gaba_types, 'P21 Mapping Prob', obs_id_split='Type')\n",
    "clust_obs_plot(P22_gaba_types, 'pct_counts_mt', obs_id_split='Type')\n",
    "clust_obs_plot(P22_gaba_types, 'Doublet Score', obs_id_split='Type')\n",
    "clust_obs_plot(P22_gaba_types, 'total_counts', obs_id_split='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to V1 P21 multiome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gluta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P21_V1 = rem_fem(sc.read_h5ad('P21full_glut_v3.h5ad'))\n",
    "P21_V1 = P21_V1[P21_V1.obs['Study']!='2022 RNA',:].copy()\n",
    "P21_V1.obs['Type'] = P21_V1.obs['Type_nn_dists']\n",
    "P21_V1.X = P21_V1.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut = sc.read_h5ad('h5ads/P22Sham_glut_v2.h5ad')\n",
    "P22_glut.X = P22_glut.raw.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The change below to plot_conf_matx() fixes the indeing issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#code\n",
    "#import\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from harmony import harmonize\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.utils import shuffle\n",
    "from anndata import AnnData\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Union, Optional, Tuple, Collection, Sequence, Iterable\n",
    "from scipy.stats import hypergeom\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sn\n",
    "from random import sample\n",
    "import pickle\n",
    "#from matplotlib_venn import venn3, venn3_circles\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from sklearn.metrics import confusion_matrix, adjusted_rand_score, roc_curve, auc, classification_report, f1_score, cohen_kappa_score\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle, islice\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import scanpy as sc\n",
    "sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.set_figure_params(dpi=100, dpi_save=200)\n",
    "\n",
    "\n",
    "import scrublet as scr\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "sc.settings.verbosity = 0           # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_header()\n",
    "sc.set_figure_params(dpi=75, dpi_save=200)\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def test_func():\n",
    "    print('test')\n",
    "    \n",
    "def test2():\n",
    "    print('test2')\n",
    "def rem_fem(adata):\n",
    "    for j in list(adata.obs.Type_nn_dists.values.categories):\n",
    "        if ('Fem' in j):\n",
    "            adata = adata[adata.obs['Type_nn_dists']!=j,:]\n",
    "            adata = adata[adata.obs['Type_leiden']!=j,:]\n",
    "            adata = adata[adata.obs['Type']!=j,:]\n",
    "    return adata\n",
    "def pipeline_short(adata, batch_correct, batch_ID):\n",
    "                   \n",
    "    if sp.sparse.issparse(adata.X):\n",
    "        if np.any(adata.X.A<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(adata.X<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "\n",
    "    \n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key=batch_ID) #HVGs\n",
    "    adata.raw = adata\n",
    "    sc.pp.scale(adata, max_value=10) #scale\n",
    "    sc.tl.pca(adata, svd_solver='arpack') #run PCA\n",
    "    \n",
    "    if (batch_correct):\n",
    "        Z = harmonize(adata.obsm['X_pca'], adata.obs, batch_key = batch_ID)\n",
    "        adata.obsm['X_harmony'] = Z\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_harmony', n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "\n",
    "    else:\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "        \n",
    "def pre_process(adata, batch_ID):\n",
    "    \n",
    "    if sp.sparse.issparse(adata.X):\n",
    "        if np.any(adata.X.A<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(adata.X<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    \n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key=batch_ID) #HVGs\n",
    "    adata.raw = adata\n",
    "    sc.pp.scale(adata, max_value=10) #scale\n",
    "    sc.tl.pca(adata, svd_solver='arpack') #run PCA\n",
    "    \n",
    "\n",
    "def pipeline(adata, \n",
    "             batch_correct: Optional[bool] = None,\n",
    "             batch_ID: Optional[str] = None):\n",
    "    \n",
    "    if (batch_correct):\n",
    "        Z = harmonize(adata.obsm['X_pca'], adata.obs, batch_key = batch_ID)\n",
    "        adata.obsm['X_harmony'] = Z\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_harmony', n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "\n",
    "    else:\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "\n",
    "def clust_obs_plot(adata, obs_id, obs_id_split):\n",
    "\n",
    "    obs_quants = []\n",
    "    for i in adata.obs[obs_id_split].values.categories:\n",
    "    #for i in dr_corr.obs['Atlas Label'].values.categories:\n",
    "        obs_quants.append(np.mean(adata[adata.obs[obs_id_split]==i].obs[obs_id].values))\n",
    "        \n",
    "    x, y = adata.obs[obs_id_split].values.categories, obs_quants\n",
    "    ser = pd.Series(y,x).sort_values(ascending=False)\n",
    "    \n",
    "    x,y = ser.index, ser.values\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.bar(x,y)\n",
    "    plt.axhline(np.median(adata.obs[obs_id]), color='r')\n",
    "    plt.ylabel(obs_id)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def DE(adata, obs_id, obs_id_test, ref, pts_thresh, lf_thresh):\n",
    "\n",
    "    sc.tl.rank_genes_groups(adata, groupby=obs_id, groups=[obs_id_test], \n",
    "                                reference=ref, method='t-test', pts=True, use_raw=True)\n",
    "\n",
    "    lfcs = adata.uns['rank_genes_groups']['logfoldchanges'].astype([(obs_id_test, '<f8')]).view('<f8') \n",
    "\n",
    "    l231_genes = adata.uns['rank_genes_groups']['pts']\n",
    "\n",
    "    lfcs = []\n",
    "    p_adj = []\n",
    "    names = list(adata.uns['rank_genes_groups']['names'].astype([(obs_id_test, '<U50')]).view('<U50'))\n",
    "    logfoldchanges = adata.uns['rank_genes_groups']['logfoldchanges'].astype([(obs_id_test, '<f8')]).view('<f8')\n",
    "    pvals_adj = adata.uns['rank_genes_groups']['pvals_adj'].astype([(obs_id_test, '<f8')]).view('<f8')\n",
    "\n",
    "    for i in l231_genes.index:\n",
    "        lfcs.append(logfoldchanges[names.index(i)])\n",
    "        p_adj.append(pvals_adj[names.index(i)])\n",
    "\n",
    "    l231_genes['LF'] = lfcs\n",
    "    l231_genes['p_adj'] = p_adj\n",
    "    \n",
    "    #plt.hist(l231_genes[obs_id_test].values)\n",
    "\n",
    "    l231_genes = l231_genes[l231_genes[obs_id_test]>pts_thresh]\n",
    "    \n",
    "    sort_LF = l231_genes.sort_values('LF', ascending=False)\n",
    "    \n",
    "    a = np.where(sort_LF[sort_LF['LF']>0.6]['p_adj'].values<0.05)[0].shape[0]\n",
    "    b = sort_LF[sort_LF['LF']>0.6].shape[0]\n",
    "    if(a == b):\n",
    "        print('cutoffs are good at 1.5 FC level')\n",
    "    else:\n",
    "        print(a,b)\n",
    "        \n",
    "    return sort_LF[sort_LF['LF']>lf_thresh]\n",
    "        \n",
    "def clust_obs(adata, obs_id, obs_id_split):\n",
    "\n",
    "    obs_quants = []\n",
    "    for i in adata.obs[obs_id_split].values.categories:\n",
    "    #for i in dr_corr.obs['Atlas Label'].values.categories:\n",
    "        obs_quants.append(np.mean(adata[adata.obs[obs_id_split]==i].obs[obs_id].values))\n",
    "        \n",
    "    x, y = adata.obs[obs_id_split].values.categories, obs_quants\n",
    "    ser = pd.Series(y,x).sort_values(ascending=False)\n",
    "    return ser.index, ser.values\n",
    "\n",
    "def res_tune(adata, def_labels):\n",
    "    \n",
    "    adata_ari = []\n",
    "    adata_num_clusts = []\n",
    "    for res in np.arange(1,2.1, 0.1):\n",
    "        sc.tl.leiden(adata, resolution=res)\n",
    "\n",
    "        adata_num_clusts.append(len(adata.obs.leiden.values.categories))\n",
    "        adata_ari.append(adjusted_rand_score(def_labels, list(adata.obs.leiden.values)))\n",
    "        \n",
    "    return adata_num_clusts, adata_ari\n",
    "\n",
    "from math import log, e\n",
    "def entropy2(labels, base=None):\n",
    "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "  n_labels = len(labels)\n",
    "\n",
    "  if n_labels <= 1:\n",
    "    return 0\n",
    "\n",
    "  value,counts = np.unique(labels, return_counts=True)\n",
    "  probs = counts / n_labels\n",
    "  n_classes = np.count_nonzero(probs)\n",
    "\n",
    "  if n_classes <= 1:\n",
    "    return 0\n",
    "\n",
    "  ent = 0.\n",
    "\n",
    "  # Compute entropy\n",
    "  base = e if base is None else base\n",
    "  for i in probs:\n",
    "    ent -= i * log(i, base)\n",
    "\n",
    "  return ent\n",
    "\n",
    "def check_clust(adata, clus, ref_age):\n",
    "    sc.pl.umap(adata[adata.obs.leiden==clus], color=[ref_age+'-2022 Mapping Prob', ref_age+'-2022 Mapping Label'])\n",
    "    print(adata[adata.obs.leiden==clus].shape, 'mean prob', np.mean(adata[adata.obs.leiden==clus].obs[ref_age+'-2022 Mapping Prob']))\n",
    "    ser = adata[adata.obs.leiden==clus].obs['Sample'].value_counts()\n",
    "    plt.bar(ser.index, ser.values,)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    ser = adata[adata.obs.leiden==clus].obs[ref_age+'-2022 Mapping Label'].value_counts()\n",
    "    plt.bar(ser.index, ser.values,)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "def maj_vote_annot(adata, clust_id, putative_annot, final_annot, vlmcendo=False):\n",
    "    clust_list = []\n",
    "    for i in adata.obs[clust_id].values.categories:\n",
    "        clust = adata[adata.obs[clust_id]==i,:]\n",
    "        clust_df = clust.obs[putative_annot].value_counts()\n",
    "        if (clust_df.index[0]=='Unassigned'):\n",
    "            biggest_cat = clust_df.index[1]\n",
    "        else: biggest_cat = clust_df.index[0]\n",
    "        if (vlmcendo):\n",
    "            if (biggest_cat in ('VLMC', 'Endo')):\n",
    "                clust.obs[putative_annot+'_maj'] = ['VLMC+Endo']*clust.shape[0]\n",
    "                clust_list.append(clust)\n",
    "            else:\n",
    "                clust.obs[putative_annot+'_maj'] = [biggest_cat]*clust.shape[0]\n",
    "                clust_list.append(clust)\n",
    "        else:\n",
    "            clust.obs[putative_annot+'_maj'] = [biggest_cat]*clust.shape[0]\n",
    "            clust_list.append(clust)\n",
    "\n",
    "    adata_annot = clust_list[0].concatenate(clust_list[1:], index_unique=None)\n",
    "    adata_annot.obs[final_annot] = adata_annot.obs[putative_annot+'_maj']\n",
    "    a = sc.pl.dotplot(adata_annot, 'Mdga1', 'leiden',) #to fix categories\n",
    "    return adata_annot\n",
    "\n",
    "def clus_sample_bars(adata, a_, b_, samp_id, clus_id, size=(13,9.5)):\n",
    "    adata_ser = adata.obs[samp_id].value_counts(normalize=True)\n",
    "    plt.bar(adata_ser.index, adata_ser.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Overall Dataset')\n",
    "    plt.show()\n",
    "    a,b = 0 ,0 \n",
    "    fig, axs = plt.subplots(a_,b_, figsize=size)\n",
    "\n",
    "    for i in adata.obs[clus_id].values.categories:\n",
    "        clus = adata[adata.obs[clus_id]==i,:]\n",
    "        clus_ser = clus.obs[samp_id].value_counts(normalize=True)\n",
    "        axs[a,b].bar(clus_ser.index, clus_ser.values)\n",
    "        axs[a,b].set_title(i+' ('+str(clus.shape[0])+' cells)')\n",
    "        axs[a,b].set_ylabel('Fraction')\n",
    "        axs[a,b].set_xticklabels(clus_ser.index, rotation=90)\n",
    "        b = b + 1\n",
    "        if (b>=b_):\n",
    "            b = 0\n",
    "            a = a + 1\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "def euclidean_distance(vector1, vectors_list):\n",
    "    # Ensure that the input vectors are numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vectors_list = [np.array(vector) for vector in vectors_list]\n",
    "    \n",
    "    # Calculate the Euclidean distance between the input vector and each vector in the list\n",
    "    distances = [np.linalg.norm(vector1 - vector) for vector in vectors_list]\n",
    "    return distances\n",
    "\n",
    "def nn_voting(adata, type_old, type_new, delta_thresh):\n",
    "    adata.obs['idx'] = np.arange(adata.shape[0])\n",
    "\n",
    "#     #re-make graph using UMAP space for voting below\n",
    "#     sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_umap')\n",
    "#     sc.tl.umap(adata)\n",
    "\n",
    "    adata.obs[type_new] = adata.obs[type_old]\n",
    "    adata_un_idx = adata[adata.obs[type_old]=='Unassigned',:].obs.idx.values #indices of unassigned cells\n",
    "    neigh_matx = adata.obsp['distances'].A #each row's nonzero entries tells neighbors\n",
    "    adata_iGBs = list(adata.obs[type_old].values) #all the step1 assignments\n",
    "    n_neighbs = adata.uns['neighbors']['params']['n_neighbors']\n",
    "    print(adata.uns['neighbors']['params'])\n",
    "    print(' ')\n",
    "    \n",
    "    print(\"Pre-voting unassigned: \", adata_iGBs.count('Unassigned'),\n",
    "          adata_iGBs.count('Unassigned')/adata.shape[0])\n",
    "    \n",
    "    \n",
    "    delta = 1 #init delta\n",
    "    while (delta>delta_thresh):    \n",
    "        un_frac1 = adata_iGBs.count('Unassigned')/adata.shape[0] #frac of unassigned cells before voting\n",
    "\n",
    "        #loop thru each cell\n",
    "        for i in adata_un_idx:\n",
    "\n",
    "            #so that it only loops thru still-unassigned cells after first pass\n",
    "            if (adata.obs[type_new][i]=='Unassigned'):\n",
    "                neighbs_idx = np.where(neigh_matx[i,:]>0)[0] #cell i's neighbors\n",
    "                neighbs_iGBs = adata[neighbs_idx].obs[type_new] #the neighbors' iGBs\n",
    "\n",
    "#                 #if there's a type in the neighbors that is majority, assign it\n",
    "#                 if (neighbs_iGBs.value_counts()[0] > n_neighbs/2):\n",
    "#                     adata_iGBs[i] = neighbs_iGBs.value_counts().index[0]\n",
    "                \n",
    "                #no need to be majority, just be biggest one\n",
    "                adata_iGBs[i] = neighbs_iGBs.value_counts().index[0]\n",
    "                #update the IDs to help assignment of next cell\n",
    "                adata.obs[type_new] =  pd.Categorical(adata_iGBs)\n",
    "\n",
    "        un_frac2 = adata_iGBs.count('Unassigned')/adata.shape[0] #frac of unassigned cells after voting\n",
    "\n",
    "        delta = (un_frac1-un_frac2)/un_frac1   #stop when this changes by less than 1%\n",
    "        print(delta, un_frac2)\n",
    "\n",
    "    print(\"Post-voting unassigned: \", adata_iGBs.count('Unassigned'),\n",
    "          adata_iGBs.count('Unassigned')/adata.shape[0])\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "def make_hmap(adata,thres, y_obs='Type', x_obs='leiden', ):\n",
    "    crosstab_data = pd.crosstab(adata.obs[y_obs], adata.obs[x_obs], \n",
    "                           normalize='index')\n",
    "    # For rows\n",
    "    row_order = np.argsort(-crosstab_data.values.max(axis=1))\n",
    "\n",
    "    # For columns\n",
    "    column_order = np.argsort(-crosstab_data.values.max(axis=0))\n",
    "\n",
    "    reordered_rows = crosstab_data.index[row_order]\n",
    "    reordered_crosstab_rows = crosstab_data.iloc[row_order]\n",
    "\n",
    "    # For columns\n",
    "    reordered_columns = crosstab_data.columns[column_order]\n",
    "    reordered_crosstab = reordered_crosstab_rows[reordered_columns]\n",
    "    #return reordered_crosstab\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "    \n",
    "\n",
    "    labels_true = adata.obs[y_obs]\n",
    "    labels_pred = adata.obs[x_obs]\n",
    "\n",
    "    # Compute the Adjusted Rand Index\n",
    "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "    \n",
    "    return [reordered_crosstab.applymap(lambda value: value if value > thres else np.round(value, 2)),\n",
    "           ari]\n",
    "def freq_scatter(x, y, x_lab, y_lab, hue_, unity_lim, low_lim):\n",
    "\n",
    "    if (x.shape[0]!=y.shape[0]): \n",
    "        print('x and y are different by', np.abs(x.shape[0]-y.shape[0]), 'categories')\n",
    "    \n",
    "    if (y.shape[0]>x.shape[0]):\n",
    "        y = y[x.index]\n",
    "    else:\n",
    "        x = x[y.index]\n",
    "\n",
    "    df = pd.DataFrame(index=list(x.index.values), columns=[x_lab, y_lab],\n",
    "                      data=np.transpose(np.array([x.values, y.values])))\n",
    "\n",
    "    df[hue_] = df.index\n",
    "    sn.scatterplot(data=df, x=x_lab, y = y_lab, hue=hue_, s=50)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1.03), loc='upper left', fontsize=14, ncol=2)\n",
    "    plt.plot(np.linspace(low_lim,unity_lim), np.linspace(low_lim,unity_lim), ls='--', \n",
    "             color='black', linewidth=0.75)\n",
    "    plt.title('Pearson R: '+str(np.round(sp.stats.pearsonr(x, y)[0], 3)))\n",
    "    plt.grid(False)\n",
    "    #plt.loglog()\n",
    "    \n",
    "\n",
    "\n",
    "from anndata import AnnData\n",
    "from typing import Union, Optional, Tuple, Collection, Sequence, Iterable\n",
    "\n",
    "def module_score(adata:AnnData, genes_use: list, score_name: Optional[str] = None, verbose: bool = True):\n",
    "    \n",
    "    \"\"\"\\\n",
    "    Compute module scores for all cells in adata as described in methods of RGC-dev paper.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        The (annotated) data matrix of shape `n_obs` × `n_vars`.\n",
    "        Rows correspond to cells and columns to genes.\n",
    "    genes_use\n",
    "        list of genes in module of interest\n",
    "    score_name\n",
    "        Name endowed to the module score to be computed\n",
    "        e.g. \"Mod1\"\n",
    "    verbose\n",
    "        Inform user of fraction of module genes that are in adata\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata with a new .obs called score_name\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if (score_name==None):\n",
    "        score_name = str(input(\"Provide a name for this score (no spaces): \"))\n",
    "        \n",
    "    genes_use0 = genes_use\n",
    "    genes_use = list(set(genes_use).intersection(adata.var_names))#genes that are both in module and `adata`\n",
    "    \n",
    "     \n",
    "    if (len(genes_use) == 0):\n",
    "        raise ValueError(\"Error : Must provide a list of genes that are present in the data\")\n",
    "        \n",
    "    \n",
    "    if (verbose):\n",
    "        if(len(genes_use0) > len(genes_use)):\n",
    "            n = len(genes_use0) - len(genes_use)\n",
    "            print(score_name,\": Note that\", n, \"of the\", len(genes_use0), \"genes in your module do not exist in the data set.\" )\n",
    "        else:\n",
    "            print(score_name,\": Note that all of the\", len(genes_use), \"genes in your module are in the data set.\" )\n",
    "    \n",
    "    \n",
    "    \n",
    "    adata_score = adata.copy()\n",
    "    adata_score = adata[:,genes_use]\n",
    "    \n",
    "    counts_modgenes = adata_score.X.toarray() #all cells, module genes\n",
    "    counts_all = adata.X.toarray() #all cells, all genes\n",
    "    #scores = np.mean(counts_modgenes, axis=1) - np.mean(counts_all, axis=1) #(row means of counts_modgenes ) - (row means of counts_all)\n",
    "    scores = np.mean(counts_modgenes, axis=1) #(row means of counts_modgenes )\n",
    "\n",
    "    adata.obs[score_name] = scores\n",
    "    \n",
    "    return genes_use    \n",
    "    \n",
    "\n",
    "def clean_subclasses(adata, rep):\n",
    "    #define avg PC position of each type. Gotta use a type with no \"Unassigned\" group\n",
    "    typ_mean_dict = {}\n",
    "    for i in adata.obs['Subclass'].values.categories:\n",
    "        typ_mean_dict[i] = np.mean(adata[adata.obs['Subclass']==i,:].obsm[rep][:,0:40], axis=0)\n",
    "\n",
    "    #assign based on proximity to avg type\n",
    "    dists_list = []\n",
    "    dict_keys = list(typ_mean_dict.keys())\n",
    "    for i in range(adata.shape[0]):\n",
    "        typ_ = adata.obs['Subclass'][i]\n",
    "        dists = euclidean_distance(adata.obsm[rep][i,0:40], list(typ_mean_dict.values()))\n",
    "\n",
    "        #dist = dists[dict_keys.index(typ_)]\n",
    "        typ = dict_keys[np.argmin(dists)]\n",
    "        #types.append(typ)\n",
    "        if (typ!=typ_):\n",
    "            dists_list.append(100)\n",
    "        else: dists_list.append(0)\n",
    "\n",
    "    adata.obs['Dist to Subclass'] = dists_list\n",
    "    \n",
    "def make_dict(adata, obs_id):\n",
    "\n",
    "    adata_dict = {}\n",
    "    for num,i in enumerate(adata.obs[obs_id].values.categories):\n",
    "        adata_dict[i] = num\n",
    "        \n",
    "    #adata_dict['Unassigned'] = num + 1\n",
    "    \n",
    "    return adata_dict\n",
    "    \n",
    "\n",
    "def plotConfusionMatrix(\n",
    "    ytrue,\n",
    "    ypred,\n",
    "    type,\n",
    "    xaxislabel,\n",
    "    yaxislabel,\n",
    "    title,\n",
    "    train_dict,\n",
    "    test_dict=None,\n",
    "    re_order=None,\n",
    "    re_order_cols = None,\n",
    "    re_index = None,\n",
    "    re_order_rows = None,\n",
    "    save_as=None,):\n",
    "    \n",
    "    #very bad\n",
    "    numbertrainclasses = len(set(ypred))\n",
    "    numbertestclasses = len(set(ytrue))\n",
    "    \n",
    "    #cfm is 11x11 b/c 11 is = y_true U y_pred\n",
    "    confusionmatrix = confusion_matrix(y_true = ytrue, y_pred = ypred)\n",
    "        \n",
    "    #only need this when mapping b/c if validaiton, all classes will be used and cfm will be constructed properly\n",
    "    if type == 'mapping':\n",
    "        rows = np.where(np.sum(confusionmatrix, axis=1)>0)[0]\n",
    "        cols = np.where(np.sum(confusionmatrix, axis=0)>0)[0]\n",
    "        \n",
    "        cfm = confusionmatrix[rows,:][:,cols]\n",
    "        \n",
    "        #show all columns, even ones with no mapping\n",
    "        \n",
    "        #this was changed 10/12/2023 to address unassigned issue\n",
    "        cfm_z = np.zeros((len(test_dict),len(train_dict)))\n",
    "        #cfm_z = np.zeros((len(rows),len(cols)))\n",
    "\n",
    "        cfm_z[:, np.array(pd.Categorical(ypred).categories, dtype='int')]=cfm \n",
    "        confusionmatrix = cfm_z\n",
    "        \n",
    "        #always keep only as many as rows as num of test classes\n",
    "        #but, b/c of python's 0 indexing, if the number of training classes is in the \n",
    "        #y_pred list, then that means there was an unassigned\n",
    "#       if numbertrainclasses in ypred:\n",
    "#         confusionmatrix = confusionmatrix[0:numbertestclasses,0:numbertrainclasses+1]#for Unassigned\n",
    "#       else:\n",
    "#         confusionmatrix = confusionmatrix[0:numbertestclasses,0:numbertrainclasses]\n",
    "    \n",
    "        confmatpercent = confusionmatrix/np.sum(confusionmatrix, axis=1).reshape(-1,1)\n",
    "\n",
    "        conf_df = pd.DataFrame(confmatpercent)\n",
    "        conf_df.index = list(test_dict.keys())\n",
    "\n",
    "        #name columns of conf mat\n",
    "        if(len(conf_df.columns)>len(train_dict)):\n",
    "            conf_df.columns = list(train_dict.keys())+['Unassigned']\n",
    "        else:\n",
    "            conf_df.columns = list(train_dict.keys())\n",
    "\n",
    "\n",
    "        if (re_order):\n",
    "            conf_df = conf_df[re_order_cols]\n",
    "        if (re_index):\n",
    "            conf_df = conf_df.reindex(re_order_rows)\n",
    "\n",
    "        diagcm = conf_df.to_numpy()\n",
    "    \n",
    "    \n",
    "        xticksactual = list(conf_df.columns)\n",
    "        \n",
    "        #print(conf_df)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        confmatpercent = np.zeros(confusionmatrix.shape)\n",
    "        for i in range(confusionmatrix.shape[0]):\n",
    "            if np.sum(confusionmatrix[i,:]) != 0:\n",
    "                confmatpercent[i,:] = confusionmatrix[i,:]/np.sum(confusionmatrix[i,:])\n",
    "            else:\n",
    "                confmatpercent[i,:] = confusionmatrix[i,:]\n",
    "            diagcm = confmatpercent\n",
    "            xticks = np.linspace(0, confmatpercent.shape[1]-1, confmatpercent.shape[1], dtype = int)\n",
    "        xticksactual = []\n",
    "        for i in xticks:\n",
    "            if i != numbertrainclasses:\n",
    "                xticksactual.append(list(train_dict.keys())[i])\n",
    "            else:\n",
    "                xticksactual.append('Unassigned')\n",
    "        \n",
    "    dot_max = np.max(diagcm.flatten())\n",
    "    dot_min = 0\n",
    "    if dot_min != 0 or dot_max != 1:\n",
    "        frac = np.clip(diagcm, dot_min, dot_max)\n",
    "        old_range = dot_max - dot_min\n",
    "        frac = (frac - dot_min) / old_range\n",
    "    else:\n",
    "        frac = diagcm\n",
    "    xvalues = []\n",
    "    yvalues = []\n",
    "    sizes = []\n",
    "    for i in range(diagcm.shape[0]):\n",
    "        for j in range(diagcm.shape[1]):\n",
    "            xvalues.append(j)\n",
    "            yvalues.append(i)\n",
    "            sizes.append((frac[i,j]*35)**1.5)\n",
    "    size_legend_width = 0.5\n",
    "    height = diagcm.shape[0] * 0.3 + 1\n",
    "    height = max([1.5, height])\n",
    "    heatmap_width = diagcm.shape[1] * 0.35\n",
    "    width = (\n",
    "        heatmap_width\n",
    "        + size_legend_width\n",
    "        )\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    axs = gridspec.GridSpec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        wspace=0.02,\n",
    "        hspace=0.04,\n",
    "        width_ratios=[\n",
    "                    heatmap_width,\n",
    "                    size_legend_width\n",
    "                    ],\n",
    "        height_ratios = [0.5, 10]\n",
    "        )\n",
    "    dot_ax = fig.add_subplot(axs[1, 0])\n",
    "    dot_ax.scatter(xvalues,yvalues, s = sizes, c = 'blue', norm=None, edgecolor='none')\n",
    "    y_ticks = range(diagcm.shape[0])\n",
    "    dot_ax.set_yticks(y_ticks)\n",
    "    if type == 'validation':\n",
    "        dot_ax.set_yticklabels(list(train_dict.keys()))\n",
    "    elif type == 'mapping':\n",
    "      #dot_ax.set_yticklabels(list(test_dict.keys()))\n",
    "        dot_ax.set_yticklabels(list(conf_df.index))\n",
    "    x_ticks = range(diagcm.shape[1])\n",
    "    dot_ax.set_xticks(x_ticks)\n",
    "    dot_ax.set_xticklabels(xticksactual, rotation=90)\n",
    "    dot_ax.tick_params(axis='both', labelsize='small')\n",
    "    dot_ax.grid(True, linewidth = 0.2)\n",
    "    dot_ax.set_axisbelow(True)\n",
    "    dot_ax.set_xlim(-0.5, diagcm.shape[1] + 0.5)\n",
    "    ymin, ymax = dot_ax.get_ylim()\n",
    "    dot_ax.set_ylim(ymax + 0.5, ymin - 0.5)\n",
    "    dot_ax.set_xlim(-1, diagcm.shape[1])\n",
    "    dot_ax.set_xlabel(xaxislabel)\n",
    "    dot_ax.set_ylabel(yaxislabel)\n",
    "    dot_ax.set_title(title)\n",
    "    size_legend_height = min(1.75, height)\n",
    "    wspace = 10.5 / width\n",
    "    axs3 = gridspec.GridSpecFromSubplotSpec(\n",
    "        2,\n",
    "        1,\n",
    "        subplot_spec=axs[1, 1],\n",
    "        wspace=wspace,\n",
    "        height_ratios=[\n",
    "                    size_legend_height / height,\n",
    "                    (height - size_legend_height) / height\n",
    "                    ]\n",
    "        )\n",
    "    diff = dot_max - dot_min\n",
    "    if 0.3 < diff <= 0.6:\n",
    "        step = 0.1\n",
    "    elif diff <= 0.3:\n",
    "        step = 0.05\n",
    "    else:\n",
    "        step = 0.2\n",
    "    fracs_legends = np.arange(dot_max, dot_min, step * -1)[::-1]\n",
    "    if dot_min != 0 or dot_max != 1:\n",
    "        fracs_values = (fracs_legends - dot_min) / old_range\n",
    "    else:\n",
    "        fracs_values = fracs_legends\n",
    "    size = (fracs_values * 35) ** 1.5\n",
    "    size_legend = fig.add_subplot(axs3[0])\n",
    "    size_legend.scatter(np.repeat(0, len(size)), range(len(size)), s=size, c = 'blue')\n",
    "    size_legend.set_yticks(range(len(size)))\n",
    "    labels = [\"{:.0%}\".format(x) for x in fracs_legends]\n",
    "    if dot_max < 1:\n",
    "        labels[-1] = \">\" + labels[-1]\n",
    "    size_legend.set_yticklabels(labels)\n",
    "    size_legend.set_yticklabels([\"{:.0%}\".format(x) for x in fracs_legends])\n",
    "    size_legend.tick_params(axis='y', left=False, labelleft=False, labelright=True)\n",
    "    size_legend.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "    size_legend.spines['right'].set_visible(False)\n",
    "    size_legend.spines['top'].set_visible(False)\n",
    "    size_legend.spines['left'].set_visible(False)\n",
    "    size_legend.spines['bottom'].set_visible(False)\n",
    "    size_legend.grid(False)\n",
    "    ymin, ymax = size_legend.get_ylim()\n",
    "    size_legend.set_ylim(ymin, ymax + 0.5)\n",
    "    \n",
    "    if (save_as is not None):\n",
    "        fig.savefig(save_as, bbox_inches = 'tight')\n",
    "\n",
    "    return diagcm, xticksactual, axs\n",
    "\n",
    "\n",
    "\n",
    "#This helper method plots validation plots in sequential order (i.e. first plot is for first batch, second plot is for second batch, etc.)\n",
    "def plot_validation_plots(validation_label_train_70, valid_predlabels_train_70, train_dict, save_as=None):\n",
    "    \n",
    "    ARI = adjusted_rand_score(labels_true = validation_label_train_70, \n",
    "                              labels_pred = valid_predlabels_train_70)\n",
    "    \n",
    "    \n",
    "    c = 0\n",
    "    for i in range(validation_label_train_70.shape[0]):\n",
    "        if (validation_label_train_70[i]!=valid_predlabels_train_70[i]):\n",
    "            c = c +1\n",
    "    acc = (1 - c/len(validation_label_train_70))*100\n",
    "    \n",
    "           \n",
    "    validationconfmat, validationxticks, validationplot = plotConfusionMatrix(\n",
    "    ytrue = validation_label_train_70,\n",
    "    ypred = valid_predlabels_train_70,\n",
    "    train_dict=train_dict,\n",
    "    type = 'validation',\n",
    "    save_as = save_as,\n",
    "    title = 'ARI = {:.3f}, Accuracy = {:.3f}'.format(ARI, acc),\n",
    "    xaxislabel = 'Predicted',\n",
    "    yaxislabel = 'True'\n",
    "    )\n",
    "\n",
    "def plot_mapping(test_labels, test_predlabels, test_dict, train_dict, \n",
    "                 xaxislabel, yaxislabel,\n",
    "                re_order=None,\n",
    "    re_order_cols = None,\n",
    "                 re_index = None,\n",
    "    re_order_rows = None, save_as=None):\n",
    "    \n",
    "    ARI = adjusted_rand_score(labels_true = test_labels, \n",
    "                              labels_pred = test_predlabels)\n",
    "    NCE = calculateNCE(labels_true = test_labels, labels_pred = test_predlabels)\n",
    "    \n",
    " \n",
    "    \n",
    "           \n",
    "    mappingconfmat, mappingxticks, mappingplot = plotConfusionMatrix(\n",
    "    ytrue = test_labels,\n",
    "    ypred = test_predlabels,\n",
    "    test_dict=test_dict,\n",
    "    train_dict=train_dict,\n",
    "    type = 'mapping',\n",
    "    save_as = save_as,\n",
    "    title = 'ARI = {:.3f}, NCE = {:.3f}'.format(ARI, NCE),\n",
    "    xaxislabel =xaxislabel,\n",
    "    yaxislabel = yaxislabel,\n",
    "        re_order=re_order,\n",
    "    re_order_cols = re_order_cols,\n",
    "        re_index = re_index,\n",
    "    re_order_rows = re_order_rows,\n",
    "    ) \n",
    "    return mappingconfmat, mappingxticks, mappingplot\n",
    "      \n",
    "      \n",
    "#This helper method uses xgboost to train classifiers.\n",
    "def trainclassifier(train_anndata, common_top_genes, obs_id, train_dict, eta,\n",
    "                    max_cells_per_ident, train_frac, min_cells_per_ident):\n",
    "    \n",
    "    if sp.sparse.issparse(train_anndata.X):\n",
    "        if np.any(train_anndata.X.A<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(train_anndata.X<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    numbertrainclasses = len(train_anndata.obs[obs_id].values.categories)\n",
    "\n",
    "    xgb_params_train = {\n",
    "            'objective':'multi:softprob',\n",
    "            'eval_metric':'mlogloss',\n",
    "            'num_class':numbertrainclasses,\n",
    "            'eta':eta,\n",
    "            'max_depth':4,\n",
    "            'subsample': 0.6}\n",
    "    nround = 200\n",
    "    #Train XGBoost on 70% of training data and validate on the remaining data\n",
    "\n",
    "\n",
    "    training_set_train_70 = []\n",
    "    validation_set_train_70 = []\n",
    "    training_label_train_70 = []\n",
    "    validation_label_train_70 = []\n",
    "\n",
    "    #loop thru classes to split for training and validation\n",
    "    for i in train_anndata.obs[obs_id].values.categories:\n",
    "        \n",
    "        #how many cells in a class\n",
    "        cells_in_clust = train_anndata[train_anndata.obs[obs_id]==i,:].obs_names #cell names\n",
    "        n = min(max_cells_per_ident,round(len(cells_in_clust)*train_frac))\n",
    "        \n",
    "        #sample 70% for training and rest for validation\n",
    "        train_temp = np.random.choice(cells_in_clust,n,replace = False)\n",
    "        validation_temp = np.setdiff1d(cells_in_clust, train_temp)\n",
    "        \n",
    "        #upsample small clusters\n",
    "        if len(train_temp) < min_cells_per_ident:\n",
    "            train_temp_bootstrap = np.random.choice(train_temp, size = min_cells_per_ident - int(len(train_temp)))\n",
    "            train_temp = np.hstack([train_temp_bootstrap, train_temp])\n",
    "        \n",
    "        #store training and validation **names** of cells in vectors, which update for every class\n",
    "        training_set_train_70 = np.hstack([training_set_train_70,train_temp])\n",
    "        validation_set_train_70 = np.hstack([validation_set_train_70,validation_temp])\n",
    "        \n",
    "        #store training and validation **labels** of cells in vectors, which update for every class\n",
    "        training_label_train_70 = np.hstack([training_label_train_70,np.repeat(train_dict[i],len(train_temp))])\n",
    "        validation_label_train_70 = np.hstack([validation_label_train_70,np.repeat(train_dict[i],len(validation_temp))])\n",
    "\n",
    "        #need train_dict b/c XGboost needs number as class labels, not words\n",
    "        #this is only deconvulted later in plotting function\n",
    "        \n",
    "    #put data in XGB format\n",
    "    X_train = train_anndata[training_set_train_70,common_top_genes].X\n",
    "    train_matrix_train_70 = xgb.DMatrix(data = X_train, label = training_label_train_70, \n",
    "                                        feature_names = common_top_genes)\n",
    "    \n",
    "    X_valid = train_anndata[validation_set_train_70,common_top_genes].X\n",
    "    validation_matrix_train_70 = xgb.DMatrix(data = X_valid, label = validation_label_train_70, \n",
    "                                             feature_names = common_top_genes)\n",
    "\n",
    "    del training_set_train_70, validation_set_train_70, training_label_train_70\n",
    "    \n",
    "    #Train on 70%\n",
    "    bst_model_train_70 = xgb.train(\n",
    "        params = xgb_params_train,\n",
    "        dtrain = train_matrix_train_70,\n",
    "        num_boost_round = nround)\n",
    "    \n",
    "    #Validate on 30%\n",
    "    #a validation_cells x numclasses matrix, with each vector containing prob association with the classes\n",
    "    validation_pred_train_70 = bst_model_train_70.predict(data = validation_matrix_train_70)\n",
    "    \n",
    "    #for each cell, go through vec of probs and take index of max prob: that's assignment\n",
    "    valid_predlabels_train_70 = np.zeros((validation_pred_train_70.shape[0]))\n",
    "    for i in range(validation_pred_train_70.shape[0]):\n",
    "        valid_predlabels_train_70[i] = np.argmax(validation_pred_train_70[i,:])\n",
    "        \n",
    "    \n",
    "    #Train on 100%\n",
    "    #Train XGBoost on the full training data\n",
    "    training_set_train_full = []\n",
    "    training_label_train_full = []\n",
    "\n",
    "    for i in train_anndata.obs[obs_id].values.categories.values:\n",
    "        train_temp = train_anndata.obs.index[train_anndata.obs[obs_id].values == i]\n",
    "        if len(train_temp) < 100:\n",
    "            train_temp_bootstrap = np.random.choice(train_temp, size = 100 - int(len(train_temp)))\n",
    "            train_temp = np.hstack([train_temp_bootstrap, train_temp])\n",
    "        \n",
    "        #indices of cells in class\n",
    "        training_set_train_full = np.hstack([training_set_train_full,train_temp])\n",
    "        \n",
    "        #labels of cells in class: [label*N_class] stacked onto previous classes\n",
    "        training_label_train_full = np.hstack([training_label_train_full,np.repeat(train_dict[i],len(train_temp))])\n",
    "\n",
    "\n",
    "    X_train_full = train_anndata[training_set_train_full,common_top_genes].X\n",
    "    full_training_data = xgb.DMatrix(data = X_train_full, label = training_label_train_full, \n",
    "                                     feature_names = common_top_genes)\n",
    "\n",
    "    del training_set_train_full, training_label_train_full\n",
    "\n",
    "    bst_model_full_train = xgb.train(\n",
    "        params = xgb_params_train,\n",
    "        dtrain = full_training_data,\n",
    "        num_boost_round = nround)\n",
    "\n",
    "    \n",
    "    \n",
    "    print('trainclassifier() complete after', np.round(time.time() - start_time), 'seconds')\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(validation_label_train_70, valid_predlabels_train_70, average = None)\n",
    "\n",
    "    \n",
    "    #real labels of validation set, predicted labels, classifier.\n",
    "    #recall these are all integers that are deconvulted later in plotting using the dicts\n",
    "    return validation_label_train_70, valid_predlabels_train_70, bst_model_full_train, f1\n",
    "\n",
    "\n",
    "#This helper method predicts the testing cluster labels.\n",
    "def predict(train_anndata, common_top_genes, bst_model_train_full, test_anndata, \n",
    "            train_obs_id, test_dict, test_obs_id):\n",
    "    \n",
    "    \n",
    "    if sp.sparse.issparse(train_anndata.X):\n",
    "        if np.any(train_anndata.X.A<0):\n",
    "            raise Exception(\"Training matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(train_anndata.X<0):\n",
    "            raise Exception(\"Training matrix contains negative values\")\n",
    "\n",
    "    if sp.sparse.issparse(test_anndata.X):\n",
    "        if np.any(test_anndata.X.A<0):\n",
    "            raise Exception(\"Testing matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(test_anndata.X<0):\n",
    "            raise Exception(\"Testing matrix contains negative values\")\n",
    "  \n",
    "    \n",
    "    #Predict the testing cluster labels\n",
    "    #how many classes mapping to \n",
    "    numbertrainclasses = len(train_anndata.obs[train_obs_id].values.categories)\n",
    "    \n",
    "    #put testing data into XGB format\n",
    "    full_testing_data = xgb.DMatrix(data = test_anndata[:,common_top_genes].X, \n",
    "                                    feature_names=common_top_genes)\n",
    "    \n",
    "    #a testing_cells x numclasses matrix, with each vector containing prob association with the classes\n",
    "    test_prediction = bst_model_train_full.predict(data = full_testing_data)\n",
    "\n",
    "    #for each cell, go through vec of probs and take index of max prob (if greater than ...): that's assignment\n",
    "\n",
    "    \n",
    "    test_predlabels = np.zeros((test_prediction.shape[0]))\n",
    "    for i in range(test_prediction.shape[0]):\n",
    "        if np.max(test_prediction[i, :]) > 1.1*(1/numbertrainclasses):\n",
    "            test_predlabels[i] = np.argmax(test_prediction[i,:])\n",
    "        \n",
    "        #\"unassigned\" is a label one larger than all b/c python begins indexing at 0\n",
    "        else:\n",
    "            test_predlabels[i] = numbertrainclasses\n",
    "        \n",
    "    test_labels = np.zeros(len(test_anndata.obs[test_obs_id].values))\n",
    "    for i,l in enumerate(test_anndata.obs[test_obs_id].values):\n",
    "        test_labels[i] = test_dict[l]\n",
    "\n",
    "    #actual labels of testing set, the labels that test set mapped to \n",
    "    return test_labels, test_predlabels, test_prediction\n",
    "\n",
    "def calculateNCE(labels_true,labels_pred):\n",
    "    X = labels_true\n",
    "    Y = labels_pred\n",
    "    contTable = confusion_matrix(X,Y)[0:len(np.unique(X)), 0:len(np.unique(Y))]\n",
    "    a = np.sum(contTable, axis = 1)\n",
    "    b = np.sum(contTable, axis = 0)\n",
    "    N = np.sum(contTable)\n",
    "    pij = contTable/N\n",
    "    pi = a/N\n",
    "    pj = b/N\n",
    "    Hyx = np.zeros(contTable.shape)\n",
    "    for i in range(contTable.shape[0]):\n",
    "        for j in range(contTable.shape[1]):\n",
    "          if pij[i,j] == 0:\n",
    "            Hyx[i,j] = 0\n",
    "          else:\n",
    "            Hyx[i,j] = pij[i,j]*np.log10(pij[i,j]/pi[i])\n",
    "    CE = -np.sum(Hyx)\n",
    "    Hyi = np.zeros(contTable.shape[1])\n",
    "    for j in range(contTable.shape[1]):\n",
    "      if pj[j] == 0:\n",
    "       Hyi[j] = 0\n",
    "      else:\n",
    "        Hyi[j] = pj[j]*np.log10(pj[j])\n",
    "    Hy = -np.sum(Hyi)\n",
    "    NCE = CE/Hy\n",
    "    return NCE\n",
    "\n",
    "def train_validate(adata, adata_cell,preproc=False):\n",
    "    if (preproc):\n",
    "        adata_cell.X = adata_cell.raw.X\n",
    "        sc.pp.highly_variable_genes(adata_cell, min_mean=0.0125, max_mean=3, min_disp=0.5) #HVGs\n",
    "\n",
    "    common_hvgs = list(set(adata[:,adata.var.highly_variable].var_names).intersection(set(adata_cell.var_names)))\n",
    "    adata_m_dict = make_dict(adata, obs_id='leiden')\n",
    "    adata_cell_dict = make_dict(adata_cell, obs_id='Subclass')\n",
    "    print(len(common_hvgs), 'Shared HVGs')\n",
    "\n",
    "    valid_truelabel_adata_cell, valid_predlabels_adata_cell, model_atlas_adata_cell = trainclassifier(train_anndata=adata_cell, \n",
    "                                                                                                 common_top_genes=common_hvgs, \n",
    "                                                                                                 obs_id='Subclass', \n",
    "                                                                                                 train_dict=adata_cell_dict, \n",
    "                                                                                                 eta=0.2,\n",
    "                                                                                                 max_cells_per_ident=1000, \n",
    "                                                                                          train_frac=0.7, \n",
    "                                                                                                 min_cells_per_ident=100)\n",
    "\n",
    "    plot_validation_plots(valid_truelabel_adata_cell, valid_predlabels_adata_cell, train_dict=adata_cell_dict)\n",
    "    \n",
    "    return model_atlas_adata_cell\n",
    "\n",
    "def pairwise_map(adata_t0, adata_t1, test_lab, train_lab, t0_dict, t1_dict, recomp_HVGs, min_cells,\n",
    "                x_lab, y_lab, union_hvgs):\n",
    "    adata_t0 = adata_t0.copy() \n",
    "    adata_t1 = adata_t1.copy() \n",
    "    adata_t0.X = adata_t0.raw.X\n",
    "    adata_t1.X = adata_t1.raw.X\n",
    "    \n",
    "    if (recomp_HVGs):\n",
    "        sc.pp.highly_variable_genes(adata_t0, min_mean=0.0125, max_mean=3, min_disp=0.5) #HVGs\n",
    "        sc.pp.highly_variable_genes(adata_t1, min_mean=0.0125, max_mean=3, min_disp=0.5) #HVGs\n",
    "        t0_hvgs = list(adata_t0[:, adata_t0.var.highly_variable].var_names)\n",
    "        t1_hvgs = list(adata_t1[:, adata_t1.var.highly_variable].var_names)\n",
    "        \n",
    "        if (union_hvgs):\n",
    "            t0_t1hvgs_ = list(set(t0_hvgs).union(t1_hvgs))\n",
    "            data_inter = list(set(adata_t1.var_names).intersection(set(adata_t0.var_names)))\n",
    "            t0_t1hvgs = list(set(data_inter).intersection(set(t0_t1hvgs_)))\n",
    "\n",
    "        else:\n",
    "            t0_t1hvgs = list(set(t0_hvgs).intersection(t1_hvgs))\n",
    "\n",
    "    \n",
    "    else:\n",
    "        t0_hvgs = list(adata_t0[:, adata_t0.var.highly_variable].var_names)\n",
    "        t1_hvgs = list(adata_t1[:, adata_t1.var.highly_variable].var_names)\n",
    "        t0_t1hvgs = list(set(t0_hvgs).intersection(t1_hvgs))\n",
    "\n",
    "    print(len(t0_t1hvgs), 'shared HVGs')\n",
    "    \n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1,model_f1 = trainclassifier(train_anndata=adata_t1, \n",
    "                                                                                                 common_top_genes=t0_t1hvgs, \n",
    "                                                                                                 obs_id=train_lab, \n",
    "                                                                                                 train_dict=t1_dict, \n",
    "                                                                                                 eta=0.2,\n",
    "                                                                                                 max_cells_per_ident=1000, \n",
    "                                                                                          train_frac=0.7, \n",
    "                                                                                                min_cells_per_ident=min_cells)\n",
    "\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=t1_dict)\n",
    "\n",
    "    test_labelst0vst1, test_predlabelst0vst1, test_prediction_t0vst1 = predict(train_anndata=adata_t1, \n",
    "                                                                     common_top_genes=model_t1.feature_names, \n",
    "                                                                     bst_model_train_full=model_t1, \n",
    "                                                                     test_anndata=adata_t0,\n",
    "                                                                     train_obs_id=train_lab, \n",
    "                                                                     test_dict=t0_dict, \n",
    "                                                                     test_obs_id=test_lab)\n",
    "\n",
    "    mappingconfmatt0vst1, mappingxtickst0vst1, mappingplott0vst1 = plot_mapping(test_labels=test_labelst0vst1, \n",
    "                 test_predlabels=test_predlabelst0vst1, \n",
    "                 test_dict=t0_dict, \n",
    "                 train_dict=t1_dict,\n",
    "                     xaxislabel=x_lab, yaxislabel=y_lab,)\n",
    "    \n",
    "    test_items = test_labelst0vst1, test_predlabelst0vst1, test_prediction_t0vst1\n",
    "    mapping_items = mappingconfmatt0vst1, mappingxtickst0vst1, mappingplott0vst1\n",
    "    \n",
    "    return test_items, mapping_items\n",
    "\n",
    "def make_colors(PX_dict):\n",
    "    PX_colors = []\n",
    "    for i in PX_dict:\n",
    "        for j in colors_subclass:\n",
    "            if (j in i):\n",
    "                PX_colors.append(colors_subclass[j])\n",
    "    return PX_colors\n",
    "\n",
    "def rem_fem(adata):\n",
    "    for j in list(adata.obs.Type_nn_dists.values.categories):\n",
    "        if ('Fem' in j):\n",
    "            adata = adata[adata.obs['Type_nn_dists']!=j,:]\n",
    "            adata = adata[adata.obs['Type_leiden']!=j,:]\n",
    "    return adata\n",
    "\n",
    "def list2dict(list_in):\n",
    "\n",
    "    adata_dict = {}\n",
    "    for num,i in enumerate(list_in):\n",
    "        adata_dict[i] = num\n",
    "        \n",
    "    #adata_dict['Unassigned'] = num + 1\n",
    "    \n",
    "    return adata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_items, mapping_items = pairwise_map(adata_t0=P21_V1, adata_t1=P22_glut,test_lab='Type',\n",
    "                                                                            train_lab='Type', \n",
    "                                                                          recomp_HVGs=True, union_hvgs=False,\n",
    "                                         t0_dict=make_dict(P21_V1, 'Type'),\n",
    "                                         t1_dict=make_dict(P22_glut, 'Type'),\n",
    "                                        min_cells=100, x_lab='P22 Sham', y_lab='P21 V1')\n",
    "\n",
    "test_labelsP12vsP22, test_predlabelsP12vsP22, test_prediction_P12vsP22 = test_items\n",
    "mappingconfmatP12vsP22, mappingxticksP12vsP22_sub, mappingplotP12vsP22 = mapping_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = make_dict(P22_glut, 'Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Categorical(test_predlabelsP12vsP22).categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No  'L5NP_C',  'L5PT_G',, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kes = ['L2/3_A', 'L2/3_B', 'L2/3_C', 'L4_A', 'L4_B', 'L4_C', 'L5IT', 'L5NP_A', 'L5NP_B', 'L5PT_A', 'L5PT_B', \n",
    "       'L5PT_C', 'L5PT_D', 'L5PT_E', 'L5PT_F', 'L6CT_A', 'L6CT_B', \n",
    "       'L6CT_C', 'L6CT_D', 'L6CT_E', 'L6IT_A', 'L6IT_B', 'L6IT_C', 'L6IT_Car3', 'L6b']\n",
    "P21_V1.obs['P22 Mapping Prob'] = np.max(test_prediction_P12vsP22, axis=1)\n",
    "P21_V1.obs['P22 Mapping Label'] = pd.Categorical(test_predlabelsP12vsP22).rename_categories(kes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_scatter(x=P22_glut.obs['Type'].value_counts(normalize=True),\n",
    "            y= P21_V1.obs['P22 Mapping Label'].value_counts(normalize=True),\n",
    "            x_lab='P22 Type Freqs', y_lab='P21 V1 Types Mapping Freq', hue_='Types', low_lim=0,unity_lim=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.obs['Sub'] = P22_glut.obs['Subclass'].cat.rename_categories(['L2/3', 'L4', 'L4/5IT', 'L5NP', \n",
    "                                                                      'L5PT', 'L6CT', 'L6IT', 'L6b'])\n",
    "P21_V1.obs['Sub'] = P21_V1.obs['Subclass'].cat.rename_categories(['L2/3', 'L4', 'L4/5IT', 'L5NP', \n",
    "                                                                      'L5PT', 'L6CT', 'L6IT', 'L6b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "lims = [0.6, 0.41, 0.25, 0.35]\n",
    "low_lims = [0.2, 0.1,0,0]\n",
    "xes = []\n",
    "yes = []\n",
    "for i in ['L2/3', 'L4', 'L5', 'L6']:\n",
    "    print(i)\n",
    "    a=P22_glut[P22_glut.obs['Sub'].str.startswith(i),:]\n",
    "    b=P21_V1[P21_V1.obs['Sub'].str.startswith(i),:]\n",
    "    \n",
    "    xx = a.obs['Type'].value_counts(normalize=True)\n",
    "    yy = b.obs['P22 Mapping Label'].value_counts(normalize=True)\n",
    "    \n",
    "    #print(b.obs['P22 Mapping Label'].value_counts())\n",
    "    \n",
    "    if (i=='L4'):\n",
    "        freq_scatter(x=xx[0:4], y=yy[0:4],\n",
    "            x_lab='P22 Type Freqs', y_lab='P21 V1 Types Mapping Freq', hue_='Types', unity_lim=lims[c], \n",
    "                 low_lim=low_lims[c])\n",
    "        oh=xx\n",
    "        my=yy\n",
    "        xes.append(xx[0:4])\n",
    "        yes.append(yy[0:4])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        oh=xx\n",
    "        my=yy\n",
    "        freq_scatter(x=xx[xx.index.str.startswith(i)],\n",
    "            y=yy[yy.index.str.startswith(i)],\n",
    "            x_lab='P22 Type Freqs', y_lab='P21 V1 Types Mapping Freq', hue_='Types', unity_lim=lims[c], \n",
    "                 low_lim=low_lims[c])\n",
    "        xes.append(xx[xx.index.str.startswith(i)])\n",
    "        yes.append(yy[yy.index.str.startswith(i)])\n",
    "        \n",
    "    plt.legend(ncol=1, bbox_to_anchor=(1,1))\n",
    "    plt.show()\n",
    "    c = c +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GABA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P21_V1g = rem_fem(sc.read_h5ad('P21full_gaba_v3.h5ad'))\n",
    "P21_V1g = P21_V1g[P21_V1g.obs['Study']!='2022 RNA',:].copy()\n",
    "P21_V1g.obs['Type'] = P21_V1g.obs['Type_nn_dists']\n",
    "P21_V1g.X = P21_V1g.raw.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba = sc.read_h5ad('h5ads/P22Sham_gaba_v2.h5ad')\n",
    "P22_gaba.X = P22_gaba.raw.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The change below to plot_conf_matx() fixes the indeing issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#code\n",
    "#import\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from harmony import harmonize\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.utils import shuffle\n",
    "from anndata import AnnData\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Union, Optional, Tuple, Collection, Sequence, Iterable\n",
    "from scipy.stats import hypergeom\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sn\n",
    "from random import sample\n",
    "import pickle\n",
    "#from matplotlib_venn import venn3, venn3_circles\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from sklearn.metrics import confusion_matrix, adjusted_rand_score, roc_curve, auc, classification_report, f1_score, cohen_kappa_score\n",
    "import plotly.graph_objects as go\n",
    "from itertools import cycle, islice\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import scanpy as sc\n",
    "sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.set_figure_params(dpi=100, dpi_save=200)\n",
    "\n",
    "\n",
    "import scrublet as scr\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "sc.settings.verbosity = 0           # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_header()\n",
    "sc.set_figure_params(dpi=75, dpi_save=200)\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def test_func():\n",
    "    print('test')\n",
    "    \n",
    "def test2():\n",
    "    print('test2')\n",
    "def rem_fem(adata):\n",
    "    for j in list(adata.obs.Type_nn_dists.values.categories):\n",
    "        if ('Fem' in j):\n",
    "            adata = adata[adata.obs['Type_nn_dists']!=j,:]\n",
    "            adata = adata[adata.obs['Type_leiden']!=j,:]\n",
    "            adata = adata[adata.obs['Type']!=j,:]\n",
    "    return adata\n",
    "def pipeline_short(adata, batch_correct, batch_ID):\n",
    "                   \n",
    "    if sp.sparse.issparse(adata.X):\n",
    "        if np.any(adata.X.A<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(adata.X<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "\n",
    "    \n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key=batch_ID) #HVGs\n",
    "    adata.raw = adata\n",
    "    sc.pp.scale(adata, max_value=10) #scale\n",
    "    sc.tl.pca(adata, svd_solver='arpack') #run PCA\n",
    "    \n",
    "    if (batch_correct):\n",
    "        Z = harmonize(adata.obsm['X_pca'], adata.obs, batch_key = batch_ID)\n",
    "        adata.obsm['X_harmony'] = Z\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_harmony', n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "\n",
    "    else:\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "        \n",
    "def pre_process(adata, batch_ID):\n",
    "    \n",
    "    if sp.sparse.issparse(adata.X):\n",
    "        if np.any(adata.X.A<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(adata.X<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    \n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, batch_key=batch_ID) #HVGs\n",
    "    adata.raw = adata\n",
    "    sc.pp.scale(adata, max_value=10) #scale\n",
    "    sc.tl.pca(adata, svd_solver='arpack') #run PCA\n",
    "    \n",
    "\n",
    "def pipeline(adata, \n",
    "             batch_correct: Optional[bool] = None,\n",
    "             batch_ID: Optional[str] = None):\n",
    "    \n",
    "    if (batch_correct):\n",
    "        Z = harmonize(adata.obsm['X_pca'], adata.obs, batch_key = batch_ID)\n",
    "        adata.obsm['X_harmony'] = Z\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_harmony', n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "\n",
    "    else:\n",
    "        sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)\n",
    "        sc.tl.leiden(adata)\n",
    "        sc.tl.umap(adata)\n",
    "\n",
    "def clust_obs_plot(adata, obs_id, obs_id_split):\n",
    "\n",
    "    obs_quants = []\n",
    "    for i in adata.obs[obs_id_split].values.categories:\n",
    "    #for i in dr_corr.obs['Atlas Label'].values.categories:\n",
    "        obs_quants.append(np.mean(adata[adata.obs[obs_id_split]==i].obs[obs_id].values))\n",
    "        \n",
    "    x, y = adata.obs[obs_id_split].values.categories, obs_quants\n",
    "    ser = pd.Series(y,x).sort_values(ascending=False)\n",
    "    \n",
    "    x,y = ser.index, ser.values\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.bar(x,y)\n",
    "    plt.axhline(np.median(adata.obs[obs_id]), color='r')\n",
    "    plt.ylabel(obs_id)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def DE(adata, obs_id, obs_id_test, ref, pts_thresh, lf_thresh):\n",
    "\n",
    "    sc.tl.rank_genes_groups(adata, groupby=obs_id, groups=[obs_id_test], \n",
    "                                reference=ref, method='t-test', pts=True, use_raw=True)\n",
    "\n",
    "    lfcs = adata.uns['rank_genes_groups']['logfoldchanges'].astype([(obs_id_test, '<f8')]).view('<f8') \n",
    "\n",
    "    l231_genes = adata.uns['rank_genes_groups']['pts']\n",
    "\n",
    "    lfcs = []\n",
    "    p_adj = []\n",
    "    names = list(adata.uns['rank_genes_groups']['names'].astype([(obs_id_test, '<U50')]).view('<U50'))\n",
    "    logfoldchanges = adata.uns['rank_genes_groups']['logfoldchanges'].astype([(obs_id_test, '<f8')]).view('<f8')\n",
    "    pvals_adj = adata.uns['rank_genes_groups']['pvals_adj'].astype([(obs_id_test, '<f8')]).view('<f8')\n",
    "\n",
    "    for i in l231_genes.index:\n",
    "        lfcs.append(logfoldchanges[names.index(i)])\n",
    "        p_adj.append(pvals_adj[names.index(i)])\n",
    "\n",
    "    l231_genes['LF'] = lfcs\n",
    "    l231_genes['p_adj'] = p_adj\n",
    "    \n",
    "    #plt.hist(l231_genes[obs_id_test].values)\n",
    "\n",
    "    l231_genes = l231_genes[l231_genes[obs_id_test]>pts_thresh]\n",
    "    \n",
    "    sort_LF = l231_genes.sort_values('LF', ascending=False)\n",
    "    \n",
    "    a = np.where(sort_LF[sort_LF['LF']>0.6]['p_adj'].values<0.05)[0].shape[0]\n",
    "    b = sort_LF[sort_LF['LF']>0.6].shape[0]\n",
    "    if(a == b):\n",
    "        print('cutoffs are good at 1.5 FC level')\n",
    "    else:\n",
    "        print(a,b)\n",
    "        \n",
    "    return sort_LF[sort_LF['LF']>lf_thresh]\n",
    "        \n",
    "def clust_obs(adata, obs_id, obs_id_split):\n",
    "\n",
    "    obs_quants = []\n",
    "    for i in adata.obs[obs_id_split].values.categories:\n",
    "    #for i in dr_corr.obs['Atlas Label'].values.categories:\n",
    "        obs_quants.append(np.mean(adata[adata.obs[obs_id_split]==i].obs[obs_id].values))\n",
    "        \n",
    "    x, y = adata.obs[obs_id_split].values.categories, obs_quants\n",
    "    ser = pd.Series(y,x).sort_values(ascending=False)\n",
    "    return ser.index, ser.values\n",
    "\n",
    "def res_tune(adata, def_labels):\n",
    "    \n",
    "    adata_ari = []\n",
    "    adata_num_clusts = []\n",
    "    for res in np.arange(1,2.1, 0.1):\n",
    "        sc.tl.leiden(adata, resolution=res)\n",
    "\n",
    "        adata_num_clusts.append(len(adata.obs.leiden.values.categories))\n",
    "        adata_ari.append(adjusted_rand_score(def_labels, list(adata.obs.leiden.values)))\n",
    "        \n",
    "    return adata_num_clusts, adata_ari\n",
    "\n",
    "from math import log, e\n",
    "def entropy2(labels, base=None):\n",
    "  \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "  n_labels = len(labels)\n",
    "\n",
    "  if n_labels <= 1:\n",
    "    return 0\n",
    "\n",
    "  value,counts = np.unique(labels, return_counts=True)\n",
    "  probs = counts / n_labels\n",
    "  n_classes = np.count_nonzero(probs)\n",
    "\n",
    "  if n_classes <= 1:\n",
    "    return 0\n",
    "\n",
    "  ent = 0.\n",
    "\n",
    "  # Compute entropy\n",
    "  base = e if base is None else base\n",
    "  for i in probs:\n",
    "    ent -= i * log(i, base)\n",
    "\n",
    "  return ent\n",
    "\n",
    "def check_clust(adata, clus, ref_age):\n",
    "    sc.pl.umap(adata[adata.obs.leiden==clus], color=[ref_age+'-2022 Mapping Prob', ref_age+'-2022 Mapping Label'])\n",
    "    print(adata[adata.obs.leiden==clus].shape, 'mean prob', np.mean(adata[adata.obs.leiden==clus].obs[ref_age+'-2022 Mapping Prob']))\n",
    "    ser = adata[adata.obs.leiden==clus].obs['Sample'].value_counts()\n",
    "    plt.bar(ser.index, ser.values,)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    ser = adata[adata.obs.leiden==clus].obs[ref_age+'-2022 Mapping Label'].value_counts()\n",
    "    plt.bar(ser.index, ser.values,)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "def maj_vote_annot(adata, clust_id, putative_annot, final_annot, vlmcendo=False):\n",
    "    clust_list = []\n",
    "    for i in adata.obs[clust_id].values.categories:\n",
    "        clust = adata[adata.obs[clust_id]==i,:]\n",
    "        clust_df = clust.obs[putative_annot].value_counts()\n",
    "        if (clust_df.index[0]=='Unassigned'):\n",
    "            biggest_cat = clust_df.index[1]\n",
    "        else: biggest_cat = clust_df.index[0]\n",
    "        if (vlmcendo):\n",
    "            if (biggest_cat in ('VLMC', 'Endo')):\n",
    "                clust.obs[putative_annot+'_maj'] = ['VLMC+Endo']*clust.shape[0]\n",
    "                clust_list.append(clust)\n",
    "            else:\n",
    "                clust.obs[putative_annot+'_maj'] = [biggest_cat]*clust.shape[0]\n",
    "                clust_list.append(clust)\n",
    "        else:\n",
    "            clust.obs[putative_annot+'_maj'] = [biggest_cat]*clust.shape[0]\n",
    "            clust_list.append(clust)\n",
    "\n",
    "    adata_annot = clust_list[0].concatenate(clust_list[1:], index_unique=None)\n",
    "    adata_annot.obs[final_annot] = adata_annot.obs[putative_annot+'_maj']\n",
    "    a = sc.pl.dotplot(adata_annot, 'Mdga1', 'leiden',) #to fix categories\n",
    "    return adata_annot\n",
    "\n",
    "def clus_sample_bars(adata, a_, b_, samp_id, clus_id, size=(13,9.5)):\n",
    "    adata_ser = adata.obs[samp_id].value_counts(normalize=True)\n",
    "    plt.bar(adata_ser.index, adata_ser.values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Overall Dataset')\n",
    "    plt.show()\n",
    "    a,b = 0 ,0 \n",
    "    fig, axs = plt.subplots(a_,b_, figsize=size)\n",
    "\n",
    "    for i in adata.obs[clus_id].values.categories:\n",
    "        clus = adata[adata.obs[clus_id]==i,:]\n",
    "        clus_ser = clus.obs[samp_id].value_counts(normalize=True)\n",
    "        axs[a,b].bar(clus_ser.index, clus_ser.values)\n",
    "        axs[a,b].set_title(i+' ('+str(clus.shape[0])+' cells)')\n",
    "        axs[a,b].set_ylabel('Fraction')\n",
    "        axs[a,b].set_xticklabels(clus_ser.index, rotation=90)\n",
    "        b = b + 1\n",
    "        if (b>=b_):\n",
    "            b = 0\n",
    "            a = a + 1\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "def euclidean_distance(vector1, vectors_list):\n",
    "    # Ensure that the input vectors are numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vectors_list = [np.array(vector) for vector in vectors_list]\n",
    "    \n",
    "    # Calculate the Euclidean distance between the input vector and each vector in the list\n",
    "    distances = [np.linalg.norm(vector1 - vector) for vector in vectors_list]\n",
    "    return distances\n",
    "\n",
    "def nn_voting(adata, type_old, type_new, delta_thresh):\n",
    "    adata.obs['idx'] = np.arange(adata.shape[0])\n",
    "\n",
    "#     #re-make graph using UMAP space for voting below\n",
    "#     sc.pp.neighbors(adata, n_neighbors=15, use_rep='X_umap')\n",
    "#     sc.tl.umap(adata)\n",
    "\n",
    "    adata.obs[type_new] = adata.obs[type_old]\n",
    "    adata_un_idx = adata[adata.obs[type_old]=='Unassigned',:].obs.idx.values #indices of unassigned cells\n",
    "    neigh_matx = adata.obsp['distances'].A #each row's nonzero entries tells neighbors\n",
    "    adata_iGBs = list(adata.obs[type_old].values) #all the step1 assignments\n",
    "    n_neighbs = adata.uns['neighbors']['params']['n_neighbors']\n",
    "    print(adata.uns['neighbors']['params'])\n",
    "    print(' ')\n",
    "    \n",
    "    print(\"Pre-voting unassigned: \", adata_iGBs.count('Unassigned'),\n",
    "          adata_iGBs.count('Unassigned')/adata.shape[0])\n",
    "    \n",
    "    \n",
    "    delta = 1 #init delta\n",
    "    while (delta>delta_thresh):    \n",
    "        un_frac1 = adata_iGBs.count('Unassigned')/adata.shape[0] #frac of unassigned cells before voting\n",
    "\n",
    "        #loop thru each cell\n",
    "        for i in adata_un_idx:\n",
    "\n",
    "            #so that it only loops thru still-unassigned cells after first pass\n",
    "            if (adata.obs[type_new][i]=='Unassigned'):\n",
    "                neighbs_idx = np.where(neigh_matx[i,:]>0)[0] #cell i's neighbors\n",
    "                neighbs_iGBs = adata[neighbs_idx].obs[type_new] #the neighbors' iGBs\n",
    "\n",
    "#                 #if there's a type in the neighbors that is majority, assign it\n",
    "#                 if (neighbs_iGBs.value_counts()[0] > n_neighbs/2):\n",
    "#                     adata_iGBs[i] = neighbs_iGBs.value_counts().index[0]\n",
    "                \n",
    "                #no need to be majority, just be biggest one\n",
    "                adata_iGBs[i] = neighbs_iGBs.value_counts().index[0]\n",
    "                #update the IDs to help assignment of next cell\n",
    "                adata.obs[type_new] =  pd.Categorical(adata_iGBs)\n",
    "\n",
    "        un_frac2 = adata_iGBs.count('Unassigned')/adata.shape[0] #frac of unassigned cells after voting\n",
    "\n",
    "        delta = (un_frac1-un_frac2)/un_frac1   #stop when this changes by less than 1%\n",
    "        print(delta, un_frac2)\n",
    "\n",
    "    print(\"Post-voting unassigned: \", adata_iGBs.count('Unassigned'),\n",
    "          adata_iGBs.count('Unassigned')/adata.shape[0])\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "def make_hmap(adata,thres, y_obs='Type', x_obs='leiden', ):\n",
    "    crosstab_data = pd.crosstab(adata.obs[y_obs], adata.obs[x_obs], \n",
    "                           normalize='index')\n",
    "    # For rows\n",
    "    row_order = np.argsort(-crosstab_data.values.max(axis=1))\n",
    "\n",
    "    # For columns\n",
    "    column_order = np.argsort(-crosstab_data.values.max(axis=0))\n",
    "\n",
    "    reordered_rows = crosstab_data.index[row_order]\n",
    "    reordered_crosstab_rows = crosstab_data.iloc[row_order]\n",
    "\n",
    "    # For columns\n",
    "    reordered_columns = crosstab_data.columns[column_order]\n",
    "    reordered_crosstab = reordered_crosstab_rows[reordered_columns]\n",
    "    #return reordered_crosstab\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "    \n",
    "\n",
    "    labels_true = adata.obs[y_obs]\n",
    "    labels_pred = adata.obs[x_obs]\n",
    "\n",
    "    # Compute the Adjusted Rand Index\n",
    "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "    \n",
    "    return [reordered_crosstab.applymap(lambda value: value if value > thres else np.round(value, 2)),\n",
    "           ari]\n",
    "def freq_scatter(x, y, x_lab, y_lab, hue_, unity_lim, low_lim):\n",
    "\n",
    "    if (x.shape[0]!=y.shape[0]): \n",
    "        print('x and y are different by', np.abs(x.shape[0]-y.shape[0]), 'categories')\n",
    "    \n",
    "    if (y.shape[0]>x.shape[0]):\n",
    "        y = y[x.index]\n",
    "    else:\n",
    "        x = x[y.index]\n",
    "\n",
    "    df = pd.DataFrame(index=list(x.index.values), columns=[x_lab, y_lab],\n",
    "                      data=np.transpose(np.array([x.values, y.values])))\n",
    "\n",
    "    df[hue_] = df.index\n",
    "    sn.scatterplot(data=df, x=x_lab, y = y_lab, hue=hue_, s=50)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1.03), loc='upper left', fontsize=14, ncol=2)\n",
    "    plt.plot(np.linspace(low_lim,unity_lim), np.linspace(low_lim,unity_lim), ls='--', \n",
    "             color='black', linewidth=0.75)\n",
    "    plt.title('Pearson R: '+str(np.round(sp.stats.pearsonr(x, y)[0], 3)))\n",
    "    plt.grid(False)\n",
    "    #plt.loglog()\n",
    "    \n",
    "\n",
    "\n",
    "from anndata import AnnData\n",
    "from typing import Union, Optional, Tuple, Collection, Sequence, Iterable\n",
    "\n",
    "def module_score(adata:AnnData, genes_use: list, score_name: Optional[str] = None, verbose: bool = True):\n",
    "    \n",
    "    \"\"\"\\\n",
    "    Compute module scores for all cells in adata as described in methods of RGC-dev paper.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        The (annotated) data matrix of shape `n_obs` × `n_vars`.\n",
    "        Rows correspond to cells and columns to genes.\n",
    "    genes_use\n",
    "        list of genes in module of interest\n",
    "    score_name\n",
    "        Name endowed to the module score to be computed\n",
    "        e.g. \"Mod1\"\n",
    "    verbose\n",
    "        Inform user of fraction of module genes that are in adata\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adata with a new .obs called score_name\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if (score_name==None):\n",
    "        score_name = str(input(\"Provide a name for this score (no spaces): \"))\n",
    "        \n",
    "    genes_use0 = genes_use\n",
    "    genes_use = list(set(genes_use).intersection(adata.var_names))#genes that are both in module and `adata`\n",
    "    \n",
    "     \n",
    "    if (len(genes_use) == 0):\n",
    "        raise ValueError(\"Error : Must provide a list of genes that are present in the data\")\n",
    "        \n",
    "    \n",
    "    if (verbose):\n",
    "        if(len(genes_use0) > len(genes_use)):\n",
    "            n = len(genes_use0) - len(genes_use)\n",
    "            print(score_name,\": Note that\", n, \"of the\", len(genes_use0), \"genes in your module do not exist in the data set.\" )\n",
    "        else:\n",
    "            print(score_name,\": Note that all of the\", len(genes_use), \"genes in your module are in the data set.\" )\n",
    "    \n",
    "    \n",
    "    \n",
    "    adata_score = adata.copy()\n",
    "    adata_score = adata[:,genes_use]\n",
    "    \n",
    "    counts_modgenes = adata_score.X.toarray() #all cells, module genes\n",
    "    counts_all = adata.X.toarray() #all cells, all genes\n",
    "    #scores = np.mean(counts_modgenes, axis=1) - np.mean(counts_all, axis=1) #(row means of counts_modgenes ) - (row means of counts_all)\n",
    "    scores = np.mean(counts_modgenes, axis=1) #(row means of counts_modgenes )\n",
    "\n",
    "    adata.obs[score_name] = scores\n",
    "    \n",
    "    return genes_use    \n",
    "    \n",
    "\n",
    "def clean_subclasses(adata, rep):\n",
    "    #define avg PC position of each type. Gotta use a type with no \"Unassigned\" group\n",
    "    typ_mean_dict = {}\n",
    "    for i in adata.obs['Subclass'].values.categories:\n",
    "        typ_mean_dict[i] = np.mean(adata[adata.obs['Subclass']==i,:].obsm[rep][:,0:40], axis=0)\n",
    "\n",
    "    #assign based on proximity to avg type\n",
    "    dists_list = []\n",
    "    dict_keys = list(typ_mean_dict.keys())\n",
    "    for i in range(adata.shape[0]):\n",
    "        typ_ = adata.obs['Subclass'][i]\n",
    "        dists = euclidean_distance(adata.obsm[rep][i,0:40], list(typ_mean_dict.values()))\n",
    "\n",
    "        #dist = dists[dict_keys.index(typ_)]\n",
    "        typ = dict_keys[np.argmin(dists)]\n",
    "        #types.append(typ)\n",
    "        if (typ!=typ_):\n",
    "            dists_list.append(100)\n",
    "        else: dists_list.append(0)\n",
    "\n",
    "    adata.obs['Dist to Subclass'] = dists_list\n",
    "    \n",
    "def make_dict(adata, obs_id):\n",
    "\n",
    "    adata_dict = {}\n",
    "    for num,i in enumerate(adata.obs[obs_id].values.categories):\n",
    "        adata_dict[i] = num\n",
    "        \n",
    "    #adata_dict['Unassigned'] = num + 1\n",
    "    \n",
    "    return adata_dict\n",
    "    \n",
    "\n",
    "def plotConfusionMatrix(\n",
    "    ytrue,\n",
    "    ypred,\n",
    "    type,\n",
    "    xaxislabel,\n",
    "    yaxislabel,\n",
    "    title,\n",
    "    train_dict,\n",
    "    test_dict=None,\n",
    "    re_order=None,\n",
    "    re_order_cols = None,\n",
    "    re_index = None,\n",
    "    re_order_rows = None,\n",
    "    save_as=None,):\n",
    "    \n",
    "    #very bad\n",
    "    numbertrainclasses = len(set(ypred))\n",
    "    numbertestclasses = len(set(ytrue))\n",
    "    \n",
    "    #cfm is 11x11 b/c 11 is = y_true U y_pred\n",
    "    confusionmatrix = confusion_matrix(y_true = ytrue, y_pred = ypred)\n",
    "        \n",
    "    #only need this when mapping b/c if validaiton, all classes will be used and cfm will be constructed properly\n",
    "    if type == 'mapping':\n",
    "        rows = np.where(np.sum(confusionmatrix, axis=1)>0)[0]\n",
    "        cols = np.where(np.sum(confusionmatrix, axis=0)>0)[0]\n",
    "        \n",
    "        cfm = confusionmatrix[rows,:][:,cols]\n",
    "        \n",
    "        #show all columns, even ones with no mapping\n",
    "        \n",
    "        #this was changed 10/12/2023 to address unassigned issue\n",
    "        cfm_z = np.zeros((len(test_dict),len(train_dict)))\n",
    "        #cfm_z = np.zeros((len(rows),len(cols)))\n",
    "\n",
    "        cfm_z[:, np.array(pd.Categorical(ypred).categories, dtype='int')]=cfm \n",
    "        confusionmatrix = cfm_z\n",
    "        \n",
    "        #always keep only as many as rows as num of test classes\n",
    "        #but, b/c of python's 0 indexing, if the number of training classes is in the \n",
    "        #y_pred list, then that means there was an unassigned\n",
    "#       if numbertrainclasses in ypred:\n",
    "#         confusionmatrix = confusionmatrix[0:numbertestclasses,0:numbertrainclasses+1]#for Unassigned\n",
    "#       else:\n",
    "#         confusionmatrix = confusionmatrix[0:numbertestclasses,0:numbertrainclasses]\n",
    "    \n",
    "        confmatpercent = confusionmatrix/np.sum(confusionmatrix, axis=1).reshape(-1,1)\n",
    "\n",
    "        conf_df = pd.DataFrame(confmatpercent)\n",
    "        conf_df.index = list(test_dict.keys())\n",
    "\n",
    "        #name columns of conf mat\n",
    "        if(len(conf_df.columns)>len(train_dict)):\n",
    "            conf_df.columns = list(train_dict.keys())+['Unassigned']\n",
    "        else:\n",
    "            conf_df.columns = list(train_dict.keys())\n",
    "\n",
    "\n",
    "        if (re_order):\n",
    "            conf_df = conf_df[re_order_cols]\n",
    "        if (re_index):\n",
    "            conf_df = conf_df.reindex(re_order_rows)\n",
    "\n",
    "        diagcm = conf_df.to_numpy()\n",
    "    \n",
    "    \n",
    "        xticksactual = list(conf_df.columns)\n",
    "        \n",
    "        #print(conf_df)\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        confmatpercent = np.zeros(confusionmatrix.shape)\n",
    "        for i in range(confusionmatrix.shape[0]):\n",
    "            if np.sum(confusionmatrix[i,:]) != 0:\n",
    "                confmatpercent[i,:] = confusionmatrix[i,:]/np.sum(confusionmatrix[i,:])\n",
    "            else:\n",
    "                confmatpercent[i,:] = confusionmatrix[i,:]\n",
    "            diagcm = confmatpercent\n",
    "            xticks = np.linspace(0, confmatpercent.shape[1]-1, confmatpercent.shape[1], dtype = int)\n",
    "        xticksactual = []\n",
    "        for i in xticks:\n",
    "            if i != numbertrainclasses:\n",
    "                xticksactual.append(list(train_dict.keys())[i])\n",
    "            else:\n",
    "                xticksactual.append('Unassigned')\n",
    "        \n",
    "    dot_max = np.max(diagcm.flatten())\n",
    "    dot_min = 0\n",
    "    if dot_min != 0 or dot_max != 1:\n",
    "        frac = np.clip(diagcm, dot_min, dot_max)\n",
    "        old_range = dot_max - dot_min\n",
    "        frac = (frac - dot_min) / old_range\n",
    "    else:\n",
    "        frac = diagcm\n",
    "    xvalues = []\n",
    "    yvalues = []\n",
    "    sizes = []\n",
    "    for i in range(diagcm.shape[0]):\n",
    "        for j in range(diagcm.shape[1]):\n",
    "            xvalues.append(j)\n",
    "            yvalues.append(i)\n",
    "            sizes.append((frac[i,j]*35)**1.5)\n",
    "    size_legend_width = 0.5\n",
    "    height = diagcm.shape[0] * 0.3 + 1\n",
    "    height = max([1.5, height])\n",
    "    heatmap_width = diagcm.shape[1] * 0.35\n",
    "    width = (\n",
    "        heatmap_width\n",
    "        + size_legend_width\n",
    "        )\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    axs = gridspec.GridSpec(\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        wspace=0.02,\n",
    "        hspace=0.04,\n",
    "        width_ratios=[\n",
    "                    heatmap_width,\n",
    "                    size_legend_width\n",
    "                    ],\n",
    "        height_ratios = [0.5, 10]\n",
    "        )\n",
    "    dot_ax = fig.add_subplot(axs[1, 0])\n",
    "    dot_ax.scatter(xvalues,yvalues, s = sizes, c = 'blue', norm=None, edgecolor='none')\n",
    "    y_ticks = range(diagcm.shape[0])\n",
    "    dot_ax.set_yticks(y_ticks)\n",
    "    if type == 'validation':\n",
    "        dot_ax.set_yticklabels(list(train_dict.keys()))\n",
    "    elif type == 'mapping':\n",
    "      #dot_ax.set_yticklabels(list(test_dict.keys()))\n",
    "        dot_ax.set_yticklabels(list(conf_df.index))\n",
    "    x_ticks = range(diagcm.shape[1])\n",
    "    dot_ax.set_xticks(x_ticks)\n",
    "    dot_ax.set_xticklabels(xticksactual, rotation=90)\n",
    "    dot_ax.tick_params(axis='both', labelsize='small')\n",
    "    dot_ax.grid(True, linewidth = 0.2)\n",
    "    dot_ax.set_axisbelow(True)\n",
    "    dot_ax.set_xlim(-0.5, diagcm.shape[1] + 0.5)\n",
    "    ymin, ymax = dot_ax.get_ylim()\n",
    "    dot_ax.set_ylim(ymax + 0.5, ymin - 0.5)\n",
    "    dot_ax.set_xlim(-1, diagcm.shape[1])\n",
    "    dot_ax.set_xlabel(xaxislabel)\n",
    "    dot_ax.set_ylabel(yaxislabel)\n",
    "    dot_ax.set_title(title)\n",
    "    size_legend_height = min(1.75, height)\n",
    "    wspace = 10.5 / width\n",
    "    axs3 = gridspec.GridSpecFromSubplotSpec(\n",
    "        2,\n",
    "        1,\n",
    "        subplot_spec=axs[1, 1],\n",
    "        wspace=wspace,\n",
    "        height_ratios=[\n",
    "                    size_legend_height / height,\n",
    "                    (height - size_legend_height) / height\n",
    "                    ]\n",
    "        )\n",
    "    diff = dot_max - dot_min\n",
    "    if 0.3 < diff <= 0.6:\n",
    "        step = 0.1\n",
    "    elif diff <= 0.3:\n",
    "        step = 0.05\n",
    "    else:\n",
    "        step = 0.2\n",
    "    fracs_legends = np.arange(dot_max, dot_min, step * -1)[::-1]\n",
    "    if dot_min != 0 or dot_max != 1:\n",
    "        fracs_values = (fracs_legends - dot_min) / old_range\n",
    "    else:\n",
    "        fracs_values = fracs_legends\n",
    "    size = (fracs_values * 35) ** 1.5\n",
    "    size_legend = fig.add_subplot(axs3[0])\n",
    "    size_legend.scatter(np.repeat(0, len(size)), range(len(size)), s=size, c = 'blue')\n",
    "    size_legend.set_yticks(range(len(size)))\n",
    "    labels = [\"{:.0%}\".format(x) for x in fracs_legends]\n",
    "    if dot_max < 1:\n",
    "        labels[-1] = \">\" + labels[-1]\n",
    "    size_legend.set_yticklabels(labels)\n",
    "    size_legend.set_yticklabels([\"{:.0%}\".format(x) for x in fracs_legends])\n",
    "    size_legend.tick_params(axis='y', left=False, labelleft=False, labelright=True)\n",
    "    size_legend.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "    size_legend.spines['right'].set_visible(False)\n",
    "    size_legend.spines['top'].set_visible(False)\n",
    "    size_legend.spines['left'].set_visible(False)\n",
    "    size_legend.spines['bottom'].set_visible(False)\n",
    "    size_legend.grid(False)\n",
    "    ymin, ymax = size_legend.get_ylim()\n",
    "    size_legend.set_ylim(ymin, ymax + 0.5)\n",
    "    \n",
    "    if (save_as is not None):\n",
    "        fig.savefig(save_as, bbox_inches = 'tight')\n",
    "\n",
    "    return diagcm, xticksactual, axs\n",
    "\n",
    "\n",
    "\n",
    "#This helper method plots validation plots in sequential order (i.e. first plot is for first batch, second plot is for second batch, etc.)\n",
    "def plot_validation_plots(validation_label_train_70, valid_predlabels_train_70, train_dict, save_as=None):\n",
    "    \n",
    "    ARI = adjusted_rand_score(labels_true = validation_label_train_70, \n",
    "                              labels_pred = valid_predlabels_train_70)\n",
    "    \n",
    "    \n",
    "    c = 0\n",
    "    for i in range(validation_label_train_70.shape[0]):\n",
    "        if (validation_label_train_70[i]!=valid_predlabels_train_70[i]):\n",
    "            c = c +1\n",
    "    acc = (1 - c/len(validation_label_train_70))*100\n",
    "    \n",
    "           \n",
    "    validationconfmat, validationxticks, validationplot = plotConfusionMatrix(\n",
    "    ytrue = validation_label_train_70,\n",
    "    ypred = valid_predlabels_train_70,\n",
    "    train_dict=train_dict,\n",
    "    type = 'validation',\n",
    "    save_as = save_as,\n",
    "    title = 'ARI = {:.3f}, Accuracy = {:.3f}'.format(ARI, acc),\n",
    "    xaxislabel = 'Predicted',\n",
    "    yaxislabel = 'True'\n",
    "    )\n",
    "\n",
    "def plot_mapping(test_labels, test_predlabels, test_dict, train_dict, \n",
    "                 xaxislabel, yaxislabel,\n",
    "                re_order=None,\n",
    "    re_order_cols = None,\n",
    "                 re_index = None,\n",
    "    re_order_rows = None, save_as=None):\n",
    "    \n",
    "    ARI = adjusted_rand_score(labels_true = test_labels, \n",
    "                              labels_pred = test_predlabels)\n",
    "    NCE = calculateNCE(labels_true = test_labels, labels_pred = test_predlabels)\n",
    "    \n",
    " \n",
    "    \n",
    "           \n",
    "    mappingconfmat, mappingxticks, mappingplot = plotConfusionMatrix(\n",
    "    ytrue = test_labels,\n",
    "    ypred = test_predlabels,\n",
    "    test_dict=test_dict,\n",
    "    train_dict=train_dict,\n",
    "    type = 'mapping',\n",
    "    save_as = save_as,\n",
    "    title = 'ARI = {:.3f}, NCE = {:.3f}'.format(ARI, NCE),\n",
    "    xaxislabel =xaxislabel,\n",
    "    yaxislabel = yaxislabel,\n",
    "        re_order=re_order,\n",
    "    re_order_cols = re_order_cols,\n",
    "        re_index = re_index,\n",
    "    re_order_rows = re_order_rows,\n",
    "    ) \n",
    "    return mappingconfmat, mappingxticks, mappingplot\n",
    "      \n",
    "      \n",
    "#This helper method uses xgboost to train classifiers.\n",
    "def trainclassifier(train_anndata, common_top_genes, obs_id, train_dict, eta,\n",
    "                    max_cells_per_ident, train_frac, min_cells_per_ident):\n",
    "    \n",
    "    if sp.sparse.issparse(train_anndata.X):\n",
    "        if np.any(train_anndata.X.A<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(train_anndata.X<0):\n",
    "            raise Exception(\"Matrix contains negative values\")\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    numbertrainclasses = len(train_anndata.obs[obs_id].values.categories)\n",
    "\n",
    "    xgb_params_train = {\n",
    "            'objective':'multi:softprob',\n",
    "            'eval_metric':'mlogloss',\n",
    "            'num_class':numbertrainclasses,\n",
    "            'eta':eta,\n",
    "            'max_depth':4,\n",
    "            'subsample': 0.6}\n",
    "    nround = 200\n",
    "    #Train XGBoost on 70% of training data and validate on the remaining data\n",
    "\n",
    "\n",
    "    training_set_train_70 = []\n",
    "    validation_set_train_70 = []\n",
    "    training_label_train_70 = []\n",
    "    validation_label_train_70 = []\n",
    "\n",
    "    #loop thru classes to split for training and validation\n",
    "    for i in train_anndata.obs[obs_id].values.categories:\n",
    "        \n",
    "        #how many cells in a class\n",
    "        cells_in_clust = train_anndata[train_anndata.obs[obs_id]==i,:].obs_names #cell names\n",
    "        n = min(max_cells_per_ident,round(len(cells_in_clust)*train_frac))\n",
    "        \n",
    "        #sample 70% for training and rest for validation\n",
    "        train_temp = np.random.choice(cells_in_clust,n,replace = False)\n",
    "        validation_temp = np.setdiff1d(cells_in_clust, train_temp)\n",
    "        \n",
    "        #upsample small clusters\n",
    "        if len(train_temp) < min_cells_per_ident:\n",
    "            train_temp_bootstrap = np.random.choice(train_temp, size = min_cells_per_ident - int(len(train_temp)))\n",
    "            train_temp = np.hstack([train_temp_bootstrap, train_temp])\n",
    "        \n",
    "        #store training and validation **names** of cells in vectors, which update for every class\n",
    "        training_set_train_70 = np.hstack([training_set_train_70,train_temp])\n",
    "        validation_set_train_70 = np.hstack([validation_set_train_70,validation_temp])\n",
    "        \n",
    "        #store training and validation **labels** of cells in vectors, which update for every class\n",
    "        training_label_train_70 = np.hstack([training_label_train_70,np.repeat(train_dict[i],len(train_temp))])\n",
    "        validation_label_train_70 = np.hstack([validation_label_train_70,np.repeat(train_dict[i],len(validation_temp))])\n",
    "\n",
    "        #need train_dict b/c XGboost needs number as class labels, not words\n",
    "        #this is only deconvulted later in plotting function\n",
    "        \n",
    "    #put data in XGB format\n",
    "    X_train = train_anndata[training_set_train_70,common_top_genes].X\n",
    "    train_matrix_train_70 = xgb.DMatrix(data = X_train, label = training_label_train_70, \n",
    "                                        feature_names = common_top_genes)\n",
    "    \n",
    "    X_valid = train_anndata[validation_set_train_70,common_top_genes].X\n",
    "    validation_matrix_train_70 = xgb.DMatrix(data = X_valid, label = validation_label_train_70, \n",
    "                                             feature_names = common_top_genes)\n",
    "\n",
    "    del training_set_train_70, validation_set_train_70, training_label_train_70\n",
    "    \n",
    "    #Train on 70%\n",
    "    bst_model_train_70 = xgb.train(\n",
    "        params = xgb_params_train,\n",
    "        dtrain = train_matrix_train_70,\n",
    "        num_boost_round = nround)\n",
    "    \n",
    "    #Validate on 30%\n",
    "    #a validation_cells x numclasses matrix, with each vector containing prob association with the classes\n",
    "    validation_pred_train_70 = bst_model_train_70.predict(data = validation_matrix_train_70)\n",
    "    \n",
    "    #for each cell, go through vec of probs and take index of max prob: that's assignment\n",
    "    valid_predlabels_train_70 = np.zeros((validation_pred_train_70.shape[0]))\n",
    "    for i in range(validation_pred_train_70.shape[0]):\n",
    "        valid_predlabels_train_70[i] = np.argmax(validation_pred_train_70[i,:])\n",
    "        \n",
    "    \n",
    "    #Train on 100%\n",
    "    #Train XGBoost on the full training data\n",
    "    training_set_train_full = []\n",
    "    training_label_train_full = []\n",
    "\n",
    "    for i in train_anndata.obs[obs_id].values.categories.values:\n",
    "        train_temp = train_anndata.obs.index[train_anndata.obs[obs_id].values == i]\n",
    "        if len(train_temp) < 100:\n",
    "            train_temp_bootstrap = np.random.choice(train_temp, size = 100 - int(len(train_temp)))\n",
    "            train_temp = np.hstack([train_temp_bootstrap, train_temp])\n",
    "        \n",
    "        #indices of cells in class\n",
    "        training_set_train_full = np.hstack([training_set_train_full,train_temp])\n",
    "        \n",
    "        #labels of cells in class: [label*N_class] stacked onto previous classes\n",
    "        training_label_train_full = np.hstack([training_label_train_full,np.repeat(train_dict[i],len(train_temp))])\n",
    "\n",
    "\n",
    "    X_train_full = train_anndata[training_set_train_full,common_top_genes].X\n",
    "    full_training_data = xgb.DMatrix(data = X_train_full, label = training_label_train_full, \n",
    "                                     feature_names = common_top_genes)\n",
    "\n",
    "    del training_set_train_full, training_label_train_full\n",
    "\n",
    "    bst_model_full_train = xgb.train(\n",
    "        params = xgb_params_train,\n",
    "        dtrain = full_training_data,\n",
    "        num_boost_round = nround)\n",
    "\n",
    "    \n",
    "    \n",
    "    print('trainclassifier() complete after', np.round(time.time() - start_time), 'seconds')\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(validation_label_train_70, valid_predlabels_train_70, average = None)\n",
    "\n",
    "    \n",
    "    #real labels of validation set, predicted labels, classifier.\n",
    "    #recall these are all integers that are deconvulted later in plotting using the dicts\n",
    "    return validation_label_train_70, valid_predlabels_train_70, bst_model_full_train, f1\n",
    "\n",
    "\n",
    "#This helper method predicts the testing cluster labels.\n",
    "def predict(train_anndata, common_top_genes, bst_model_train_full, test_anndata, \n",
    "            train_obs_id, test_dict, test_obs_id):\n",
    "    \n",
    "    \n",
    "    if sp.sparse.issparse(train_anndata.X):\n",
    "        if np.any(train_anndata.X.A<0):\n",
    "            raise Exception(\"Training matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(train_anndata.X<0):\n",
    "            raise Exception(\"Training matrix contains negative values\")\n",
    "\n",
    "    if sp.sparse.issparse(test_anndata.X):\n",
    "        if np.any(test_anndata.X.A<0):\n",
    "            raise Exception(\"Testing matrix contains negative values\")\n",
    "    else: \n",
    "        if np.any(test_anndata.X<0):\n",
    "            raise Exception(\"Testing matrix contains negative values\")\n",
    "  \n",
    "    \n",
    "    #Predict the testing cluster labels\n",
    "    #how many classes mapping to \n",
    "    numbertrainclasses = len(train_anndata.obs[train_obs_id].values.categories)\n",
    "    \n",
    "    #put testing data into XGB format\n",
    "    full_testing_data = xgb.DMatrix(data = test_anndata[:,common_top_genes].X, \n",
    "                                    feature_names=common_top_genes)\n",
    "    \n",
    "    #a testing_cells x numclasses matrix, with each vector containing prob association with the classes\n",
    "    test_prediction = bst_model_train_full.predict(data = full_testing_data)\n",
    "\n",
    "    #for each cell, go through vec of probs and take index of max prob (if greater than ...): that's assignment\n",
    "\n",
    "    \n",
    "    test_predlabels = np.zeros((test_prediction.shape[0]))\n",
    "    for i in range(test_prediction.shape[0]):\n",
    "        if np.max(test_prediction[i, :]) > 1.1*(1/numbertrainclasses):\n",
    "            test_predlabels[i] = np.argmax(test_prediction[i,:])\n",
    "        \n",
    "        #\"unassigned\" is a label one larger than all b/c python begins indexing at 0\n",
    "        else:\n",
    "            test_predlabels[i] = numbertrainclasses\n",
    "        \n",
    "    test_labels = np.zeros(len(test_anndata.obs[test_obs_id].values))\n",
    "    for i,l in enumerate(test_anndata.obs[test_obs_id].values):\n",
    "        test_labels[i] = test_dict[l]\n",
    "\n",
    "    #actual labels of testing set, the labels that test set mapped to \n",
    "    return test_labels, test_predlabels, test_prediction\n",
    "\n",
    "def calculateNCE(labels_true,labels_pred):\n",
    "    X = labels_true\n",
    "    Y = labels_pred\n",
    "    contTable = confusion_matrix(X,Y)[0:len(np.unique(X)), 0:len(np.unique(Y))]\n",
    "    a = np.sum(contTable, axis = 1)\n",
    "    b = np.sum(contTable, axis = 0)\n",
    "    N = np.sum(contTable)\n",
    "    pij = contTable/N\n",
    "    pi = a/N\n",
    "    pj = b/N\n",
    "    Hyx = np.zeros(contTable.shape)\n",
    "    for i in range(contTable.shape[0]):\n",
    "        for j in range(contTable.shape[1]):\n",
    "          if pij[i,j] == 0:\n",
    "            Hyx[i,j] = 0\n",
    "          else:\n",
    "            Hyx[i,j] = pij[i,j]*np.log10(pij[i,j]/pi[i])\n",
    "    CE = -np.sum(Hyx)\n",
    "    Hyi = np.zeros(contTable.shape[1])\n",
    "    for j in range(contTable.shape[1]):\n",
    "      if pj[j] == 0:\n",
    "       Hyi[j] = 0\n",
    "      else:\n",
    "        Hyi[j] = pj[j]*np.log10(pj[j])\n",
    "    Hy = -np.sum(Hyi)\n",
    "    NCE = CE/Hy\n",
    "    return NCE\n",
    "\n",
    "def train_validate(adata, adata_cell,preproc=False):\n",
    "    if (preproc):\n",
    "        adata_cell.X = adata_cell.raw.X\n",
    "        sc.pp.highly_variable_genes(adata_cell, min_mean=0.0125, max_mean=3, min_disp=0.5) #HVGs\n",
    "\n",
    "    common_hvgs = list(set(adata[:,adata.var.highly_variable].var_names).intersection(set(adata_cell.var_names)))\n",
    "    adata_m_dict = make_dict(adata, obs_id='leiden')\n",
    "    adata_cell_dict = make_dict(adata_cell, obs_id='Subclass')\n",
    "    print(len(common_hvgs), 'Shared HVGs')\n",
    "\n",
    "    valid_truelabel_adata_cell, valid_predlabels_adata_cell, model_atlas_adata_cell = trainclassifier(train_anndata=adata_cell, \n",
    "                                                                                                 common_top_genes=common_hvgs, \n",
    "                                                                                                 obs_id='Subclass', \n",
    "                                                                                                 train_dict=adata_cell_dict, \n",
    "                                                                                                 eta=0.2,\n",
    "                                                                                                 max_cells_per_ident=1000, \n",
    "                                                                                          train_frac=0.7, \n",
    "                                                                                                 min_cells_per_ident=100)\n",
    "\n",
    "    plot_validation_plots(valid_truelabel_adata_cell, valid_predlabels_adata_cell, train_dict=adata_cell_dict)\n",
    "    \n",
    "    return model_atlas_adata_cell\n",
    "\n",
    "def pairwise_map(adata_t0, adata_t1, test_lab, train_lab, t0_dict, t1_dict, recomp_HVGs, min_cells,\n",
    "                x_lab, y_lab, union_hvgs):\n",
    "    adata_t0 = adata_t0.copy() \n",
    "    adata_t1 = adata_t1.copy() \n",
    "    adata_t0.X = adata_t0.raw.X\n",
    "    adata_t1.X = adata_t1.raw.X\n",
    "    \n",
    "    if (recomp_HVGs):\n",
    "        sc.pp.highly_variable_genes(adata_t0, min_mean=0.0125, max_mean=3, min_disp=0.5) #HVGs\n",
    "        sc.pp.highly_variable_genes(adata_t1, min_mean=0.0125, max_mean=3, min_disp=0.5) #HVGs\n",
    "        t0_hvgs = list(adata_t0[:, adata_t0.var.highly_variable].var_names)\n",
    "        t1_hvgs = list(adata_t1[:, adata_t1.var.highly_variable].var_names)\n",
    "        \n",
    "        if (union_hvgs):\n",
    "            t0_t1hvgs_ = list(set(t0_hvgs).union(t1_hvgs))\n",
    "            data_inter = list(set(adata_t1.var_names).intersection(set(adata_t0.var_names)))\n",
    "            t0_t1hvgs = list(set(data_inter).intersection(set(t0_t1hvgs_)))\n",
    "\n",
    "        else:\n",
    "            t0_t1hvgs = list(set(t0_hvgs).intersection(t1_hvgs))\n",
    "\n",
    "    \n",
    "    else:\n",
    "        t0_hvgs = list(adata_t0[:, adata_t0.var.highly_variable].var_names)\n",
    "        t1_hvgs = list(adata_t1[:, adata_t1.var.highly_variable].var_names)\n",
    "        t0_t1hvgs = list(set(t0_hvgs).intersection(t1_hvgs))\n",
    "\n",
    "    print(len(t0_t1hvgs), 'shared HVGs')\n",
    "    \n",
    "    validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, model_t1,model_f1 = trainclassifier(train_anndata=adata_t1, \n",
    "                                                                                                 common_top_genes=t0_t1hvgs, \n",
    "                                                                                                 obs_id=train_lab, \n",
    "                                                                                                 train_dict=t1_dict, \n",
    "                                                                                                 eta=0.2,\n",
    "                                                                                                 max_cells_per_ident=1000, \n",
    "                                                                                          train_frac=0.7, \n",
    "                                                                                                min_cells_per_ident=min_cells)\n",
    "\n",
    "    plot_validation_plots(validation_label_train_70t0vst1, valid_predlabels_train_70t0vst1, train_dict=t1_dict)\n",
    "\n",
    "    test_labelst0vst1, test_predlabelst0vst1, test_prediction_t0vst1 = predict(train_anndata=adata_t1, \n",
    "                                                                     common_top_genes=model_t1.feature_names, \n",
    "                                                                     bst_model_train_full=model_t1, \n",
    "                                                                     test_anndata=adata_t0,\n",
    "                                                                     train_obs_id=train_lab, \n",
    "                                                                     test_dict=t0_dict, \n",
    "                                                                     test_obs_id=test_lab)\n",
    "\n",
    "    mappingconfmatt0vst1, mappingxtickst0vst1, mappingplott0vst1 = plot_mapping(test_labels=test_labelst0vst1, \n",
    "                 test_predlabels=test_predlabelst0vst1, \n",
    "                 test_dict=t0_dict, \n",
    "                 train_dict=t1_dict,\n",
    "                     xaxislabel=x_lab, yaxislabel=y_lab,)\n",
    "    \n",
    "    test_items = test_labelst0vst1, test_predlabelst0vst1, test_prediction_t0vst1\n",
    "    mapping_items = mappingconfmatt0vst1, mappingxtickst0vst1, mappingplott0vst1\n",
    "    \n",
    "    return test_items, mapping_items\n",
    "\n",
    "def make_colors(PX_dict):\n",
    "    PX_colors = []\n",
    "    for i in PX_dict:\n",
    "        for j in colors_subclass:\n",
    "            if (j in i):\n",
    "                PX_colors.append(colors_subclass[j])\n",
    "    return PX_colors\n",
    "\n",
    "def rem_fem(adata):\n",
    "    for j in list(adata.obs.Type_nn_dists.values.categories):\n",
    "        if ('Fem' in j):\n",
    "            adata = adata[adata.obs['Type_nn_dists']!=j,:]\n",
    "            adata = adata[adata.obs['Type_leiden']!=j,:]\n",
    "    return adata\n",
    "\n",
    "def list2dict(list_in):\n",
    "\n",
    "    adata_dict = {}\n",
    "    for num,i in enumerate(list_in):\n",
    "        adata_dict[i] = num\n",
    "        \n",
    "    #adata_dict['Unassigned'] = num + 1\n",
    "    \n",
    "    return adata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_items, mapping_items = pairwise_map(adata_t0=P21_V1g, adata_t1=P22_gaba,test_lab='Type',\n",
    "                                                                            train_lab='Type', \n",
    "                                                                          recomp_HVGs=True, union_hvgs=False,\n",
    "                                         t0_dict=make_dict(P21_V1g, 'Type'),\n",
    "                                         t1_dict=make_dict(P22_gaba, 'Type'),\n",
    "                                        min_cells=100, x_lab='P22 Sham', y_lab='P21 V1')\n",
    "\n",
    "test_labelsP12vsP22, test_predlabelsP12vsP22, test_prediction_P12vsP22 = test_items\n",
    "mappingconfmatP12vsP22, mappingxticksP12vsP22_sub, mappingplotP12vsP22 = mapping_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = make_dict(P22_gaba, 'Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Categorical(test_predlabelsP12vsP22).categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No   'Lamp5_D', , 'Sst_D',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kes = ['Lamp5_A', 'Lamp5_B', 'Lamp5_C','Pvalb_A', 'Pvalb_B', 'Pvalb_C', 'Pvalb_D', \n",
    "       'Pvalb_E', 'Pvalb_F', 'Sst_A', 'Sst_B', 'Sst_C',  'Sst_E', 'Sst_F', 'Sst_G', \n",
    "       'Sst_H', 'Sst_I', 'Sst_J', 'Vip_A', 'Vip_B', 'Vip_C', 'Vip_D', 'Vip_E']\n",
    "P21_V1g.obs['P22 Mapping Prob'] = np.max(test_prediction_P12vsP22, axis=1)\n",
    "P21_V1g.obs['P22 Mapping Label'] = pd.Categorical(test_predlabelsP12vsP22).rename_categories(kes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_scatter(x=P22_gaba.obs['Type'].value_counts(normalize=True),\n",
    "            y= P21_V1g.obs['P22 Mapping Label'].value_counts(normalize=True),\n",
    "            x_lab='P22 Type Freqs', y_lab='P21 V1 Types Mapping Freq', hue_='Types', low_lim=0,unity_lim=0.15, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "lims = [0.4, 0.2, 0.6, 0.6]\n",
    "low_lims = [0, 0,0,0]\n",
    "xes = []\n",
    "yes = []\n",
    "for i in ['Pvalb', 'Sst', 'Vip', 'Lamp5']:\n",
    "    print(i)\n",
    "    a=P22_gaba[P22_gaba.obs['Subclass'].str.startswith(i),:]\n",
    "    b=P21_V1g[P21_V1g.obs['Subclass'].str.startswith(i),:]\n",
    "    \n",
    "    xx = a.obs['Type'].value_counts(normalize=True)\n",
    "    yy = b.obs['P22 Mapping Label'].value_counts(normalize=True)\n",
    "    \n",
    "    #print(b.obs['P22 Mapping Label'].value_counts())\n",
    "    \n",
    "    if (i=='L4'):\n",
    "        freq_scatter(x=xx[0:4], y=yy[0:4],\n",
    "            x_lab='P22 Type Freqs', y_lab='P21 V1 Types Mapping Freq', hue_='Types', unity_lim=lims[c], \n",
    "                 low_lim=low_lims[c])\n",
    "        oh=xx\n",
    "        my=yy\n",
    "        xes.append(xx[0:4])\n",
    "        yes.append(yy[0:4])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        oh=xx\n",
    "        my=yy\n",
    "        freq_scatter(x=xx[xx.index.str.startswith(i)],\n",
    "            y=yy[yy.index.str.startswith(i)],\n",
    "            x_lab='P22 Type Freqs', y_lab='P21 V1 Types Mapping Freq', hue_='Types', unity_lim=lims[c], \n",
    "                 low_lim=low_lims[c])\n",
    "        xes.append(xx[xx.index.str.startswith(i)])\n",
    "        yes.append(yy[yy.index.str.startswith(i)])\n",
    "        \n",
    "    plt.legend(ncol=1, bbox_to_anchor=(1,1))\n",
    "    plt.show()\n",
    "    c = c +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For glut, remove L5PT_F, L5PT_G, L6CT_D, and L6CT_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['L5PT_F', 'L5PT_G', 'L6CT_D', 'L6CT_E']:\n",
    "    P22_glut = P22_glut[P22_glut.obs['Type']!=i,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gaba, remove the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Lamp5_D', 'Pvalb_E', 'Sst_D', 'Vip_E']:\n",
    "    P22_gaba = P22_gaba[P22_gaba.obs['Type']!=i,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(28334 - 28090)+(4888-4332)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.write_h5ad('h5ads/P22Sham_glut_v3.h5ad')\n",
    "P22_gaba.write_h5ad('h5ads/P22Sham_gaba_v3.h5ad')\n",
    "\n",
    "P22_glut.write_h5ad('h5ads_cmprsd/P22Sham_glut_v3.h5ad')\n",
    "P22_gaba.write_h5ad('h5ads_cmprsd/P22Sham_gaba_v3.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut = sc.read_h5ad('h5ads_cmprsd/P22Sham_glut_v3.h5ad')\n",
    "P22_gaba = sc.read_h5ad('h5ads_cmprsd/P22Sham_gaba_v3.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(P22_glut.obs['Type'].cat.categories)+len(P22_gaba.obs['Type'].cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut = sc.read_h5ad('h5ads_cmprsd/P22Sham_glut_v3.h5ad')\n",
    "P22_gaba = sc.read_h5ad('h5ads_cmprsd/P22Sham_gaba_v3.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(shuffle(P22_glut), color=['Sample'],  )\n",
    "sc.pl.umap(P22_glut, color=['Type', 'Subclass', 'leiden'], legend_loc='on data',  legend_fontsize=10,\n",
    "          add_outline=True,outline_width=(0.2,0.05), s=5 , legend_fontoutline=1, legend_fontweight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_short(P22_glut, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(P22_glut, color=['Type',], legend_loc='on data',  legend_fontsize=10,\n",
    "          add_outline=True,outline_width=(0.2,0.05), legend_fontoutline=1, legend_fontweight='semibold',\n",
    "           palette=list(sn.color_palette(\"pastel6\").as_hex())+list(sn.color_palette(\"pastel\").as_hex()),\n",
    "          title='P22 Control: 28,090 Glutamatergic Neurons', frameon=True,\n",
    "          save='P22sham_gluta_FigS3.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_short(P22_gaba, batch_correct=True, batch_ID='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba.obs['Type'] = P22_gaba.obs['Type'].cat.rename_categories(['Lamp5_A', 'Lamp5_B', 'Lamp5_C', 'Pvalb_A', 'Pvalb_B', 'Pvalb_C',\n",
    "       'Pvalb_D', 'Pvalb_E', 'Sst_A', 'Sst_B', 'Sst_C', 'Sst_D', 'Sst_E',\n",
    "       'Sst_F', 'Sst_G', 'Sst_H', 'Sst_I', 'Vip_A', 'Vip_B', 'Vip_C', 'Vip_D',\n",
    "       'Vip_E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_gaba.write_h5ad('h5ads/P22Sham_gaba_v3.h5ad')\n",
    "P22_gaba.write_h5ad('h5ads_cmprsd/P22Sham_gaba_v3.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(P22_gaba, color=['Type',], legend_loc='on data',  legend_fontsize=10,\n",
    "          add_outline=True,outline_width=(0.2,0.05), legend_fontoutline=1, legend_fontweight='semibold',\n",
    "           palette=list(sn.color_palette(\"pastel6\").as_hex())+list(sn.color_palette(\"pastel\").as_hex()),\n",
    "          title='P22 Control:  4332 GABAergic Neurons', frameon=True,\n",
    "          save='P22sham_gaba_FigS3.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markers for each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P22_glut = sc.read_h5ad('h5ads_cmprsd/P22Sham_glut_v3.h5ad')\n",
    "P22_gaba = sc.read_h5ad('h5ads_cmprsd/P22Sham_gaba_v3.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_marks(subclass):\n",
    "    subclass_marks = []\n",
    "    subclass = P22_glut[P22_glut.obs['Type'].str.startswith(subclass),:].copy()\n",
    "    subclass = subclass[subclass.obs['Sample']!='S3',:]\n",
    "    for i in subclass.obs['Type'].values.categories:\n",
    "        subclass_marks.append(DE(subclass,obs_id='Type', obs_id_test=i, ref='rest', pts_thresh=0.25, lf_thresh=1))\n",
    "    return subclass_marks, subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_marks_gab(subclass):\n",
    "    subclass_marks = []\n",
    "    subclass = P22_gaba[P22_gaba.obs['Type'].str.startswith(subclass),:].copy()\n",
    "    subclass = subclass[subclass.obs['Sample']!='S3',:]\n",
    "    for i in subclass.obs['Type'].values.categories:\n",
    "        subclass_marks.append(DE(subclass,obs_id='Type', obs_id_test=i, ref='rest', pts_thresh=0.25, lf_thresh=1))\n",
    "    return subclass_marks, subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L4_marks, subclass = sub_marks('L4')\n",
    "\n",
    "marks_plot = []\n",
    "for i in L4_marks:\n",
    "    marks_plot.append(i.index[0])\n",
    "marks_plot.remove('Xkr6')\n",
    "sc.pl.dotplot(subclass, var_names=['Nell1','Etv6', 'Gabrg3'], groupby='Type',\n",
    "             save='_L4_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L23_marks, L23_sub = sub_marks('L2/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in L23_marks:\n",
    "    marks_plot.append(i.index[0])\n",
    "    print(i.index[0:10])\n",
    "sc.pl.dotplot(L23_sub, var_names=['Cdh13', 'Trpc6', 'Kcnh5'], groupby='Type',\n",
    "             save='_L23_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L5_marks, L5_sub = sub_marks('L5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in L5_marks:\n",
    "    marks_plot = marks_plot + list(i.index[0:3])\n",
    "    print(i.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marks_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L5_sub, var_names=['Deptor', 'Dkk2', \n",
    "                                 'Svil', 'Pde7b', 'Cntnap5c', \n",
    "                                  'Erg', 'Gm44593', \n",
    "                                 'Col23a1', 'Egflam',], groupby='Type',\n",
    "             save='_L5_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6_marks, L6_sub = sub_marks('L6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in L6_marks:\n",
    "    marks_plot = marks_plot + list(i.index[0:3])\n",
    "    print(i.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marks_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L6_marks[1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L6_sub, var_names=['Sema5a',  'Brinp3',\n",
    "                                 \n",
    "                                 \n",
    "                                 'Trabd2b',\n",
    "                                  'Bmpr1b', 'Epha6', \n",
    "                                 'Tafa1', 'Synpr', 'Inpp4b'], groupby='Type',\n",
    "             save='_L6_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pv_marks, pv_sub = sub_marks('Pvalb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in pv_marks:\n",
    "    marks_plot = marks_plot + list(i.index[0:4])\n",
    "    print(i.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marks_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Col16a1', 'Il1rapl2', 'Zfpm2', 'Ralyl', 'Vipr2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(pv_sub, var_names=['Col16a1', 'Il1rapl2', 'Zfpm2', 'Ralyl', 'Vipr2']\n",
    "              , groupby='Type', save='_Pvalb_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_marks, sst_sub = sub_marks('Sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in sst_marks:\n",
    "    marks_plot = marks_plot + list(i.index[0:4])\n",
    "    print(i.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marks_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Hpse','Pcsk5','Plpp4','Prkg2','Grm8', 'Pard3b', 'Pdyn','Fbn2','Chodl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_sub.obs['Type'].values.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(sst_sub, var_names=['Hpse','Pcsk5','Plpp4','Prkg2','Grm8', \n",
    "                                  'Pard3b', 'Pdyn','Fbn2','Chodl'], groupby='Type',\n",
    "             save='_Sst_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lamp5_marks, Lamp5_sub = sub_marks('Lamp5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in Lamp5_marks:\n",
    "    marks_plot = marks_plot + list(i.index[0:4])\n",
    "    print(i.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marks_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(Lamp5_sub, var_names=['Htr1f', \n",
    "                                    'Ndnf', \n",
    "                                    'Cdh18',], \n",
    "              groupby='Type', save='_Lamp5_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Vip_marks, Vip_sub = sub_marks('Vip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "marks_plot = []\n",
    "for i in Vip_marks:\n",
    "    marks_plot = marks_plot + list(i.index[0:4])\n",
    "    print(i.index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marks_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ryr3', 'Sox5', 'Frem1', 'Grin3a' , 'Fbxo2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.dotplot(Vip_sub, var_names=['Ryr3', 'Sox5', 'Frem1', 'Grin3a' , 'Fbxo2'], groupby='Type',\n",
    "             save='_Vip_marker.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4v_marks = []\n",
    "L4v = P21_V1[P21_V1.obs['Type'].str.startswith('L4'),:].copy()\n",
    "for i in L4v.obs['Type'].values.categories:\n",
    "    L4v_marks.append(DE(L4v,obs_id='Type', obs_id_test=i, ref='rest', pts_thresh=0.4, lf_thresh=1))\n",
    "\n",
    "for i in L4v_marks:\n",
    "    sc.pl.dotplot(L4v, var_names=i.index[0:20], groupby='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(L4_marks[0].index[0:20])\n",
    "a.remove('4933424G05Rik')\n",
    "sc.pl.dotplot(L4, var_names=a, groupby='Type')\n",
    "sc.pl.dotplot(L4v, var_names=a, groupby='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(L4_marks[0].index[0:40])\n",
    "a.remove('4933424G05Rik')\n",
    "sc.pl.dotplot(L4, var_names=a, groupby='Type')\n",
    "sc.pl.dotplot(L4v, var_names=a, groupby='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pl.dotplot(L4, var_names=L4v_marks[0].index[0:20], groupby='Type')\n",
    "sc.pl.dotplot(L4v, var_names=L4v_marks[0].index[0:20], groupby='Type')\n",
    "\n",
    "sc.pl.dotplot(L4, var_names=L4v_marks[1].index[0:20], groupby='Type')\n",
    "sc.pl.dotplot(L4v, var_names=L4v_marks[1].index[0:20], groupby='Type')\n",
    "\n",
    "sc.pl.dotplot(L4, var_names=L4v_marks[2].index[0:20], groupby='Type')\n",
    "sc.pl.dotplot(L4v, var_names=L4v_marks[2].index[0:20], groupby='Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save markers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L4_marks, L4_sub = sub_marks('L4')\n",
    "L23_marks, L23_sub = sub_marks('L2/3')\n",
    "L5_marks, L5_sub = sub_marks('L5')\n",
    "L6_marks, L6_sub = sub_marks('L6')\n",
    "pv_marks, pv_sub = sub_marks_gab('Pvalb')\n",
    "sst_marks, sst_sub = sub_marks_gab('Sst')\n",
    "Vip_marks, Vip_sub = sub_marks_gab('Vip')\n",
    "Lamp5_marks, Lamp5_sub = sub_marks_gab('Lamp5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dictionary where each item is a list of DataFrames\n",
    "data_dict = {\n",
    "    'L23 Type': L23_marks,\n",
    "    'L4 Type': L4_marks,\n",
    "    'L5 Types': L5_marks,\n",
    "    'L6 Types': L6_marks,\n",
    "    'Pvalb Types': pv_marks,\n",
    "    'Sst Types': sst_marks,\n",
    "    'Vip Types': Vip_marks,\n",
    "    'Lamp5 Types': Lamp5_marks\n",
    "    \n",
    "}\n",
    "\n",
    "# Create an Excel writer object\n",
    "with pd.ExcelWriter('Tables/TableS3.xlsx') as writer:\n",
    "    for sheet_name, df_list in data_dict.items():\n",
    "        # Initialize row number for the start of each DataFrame\n",
    "        startrow = 0\n",
    "        \n",
    "        # Iterate over the list of DataFrames\n",
    "        for df in df_list:\n",
    "            # Write the DataFrame to the Excel sheet\n",
    "            df.columns =  [df.columns.tolist()[0]+' % Exp'] + df.columns.tolist()[1:]\n",
    "\n",
    "            df.to_excel(writer, sheet_name=sheet_name, startrow=startrow, index=True)\n",
    "            \n",
    "            # Update the starting row for the next DataFrame\n",
    "            startrow += len(df) + 2  # Adding 2 for some space between DataFrames\n",
    "\n",
    "# The Excel file is saved as 'output.xlsx'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xgb_env)",
   "language": "python",
   "name": "xgb_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "51.9922px",
    "width": "257.246px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "838.889px",
    "left": "108.997px",
    "top": "110.113px",
    "width": "299.497px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
